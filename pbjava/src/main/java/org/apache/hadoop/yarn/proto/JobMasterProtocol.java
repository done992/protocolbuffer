// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: hamster-protos.proto

package org.apache.hadoop.yarn.proto;

public final class JobMasterProtocol {
  private JobMasterProtocol() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  /**
   * Protobuf enum {@code AllocationStatusProto}
   */
  public enum AllocationStatusProto
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>A_ALLOCATING = 1;</code>
     */
    A_ALLOCATING(0, 1),
    /**
     * <code>A_ALLOCATION_SUCCEEDED = 2;</code>
     */
    A_ALLOCATION_SUCCEEDED(1, 2),
    /**
     * <code>A_ALLOCATION_FAILED = 3;</code>
     */
    A_ALLOCATION_FAILED(2, 3),
    ;

    /**
     * <code>A_ALLOCATING = 1;</code>
     */
    public static final int A_ALLOCATING_VALUE = 1;
    /**
     * <code>A_ALLOCATION_SUCCEEDED = 2;</code>
     */
    public static final int A_ALLOCATION_SUCCEEDED_VALUE = 2;
    /**
     * <code>A_ALLOCATION_FAILED = 3;</code>
     */
    public static final int A_ALLOCATION_FAILED_VALUE = 3;


    public final int getNumber() { return value; }

    public static AllocationStatusProto valueOf(int value) {
      switch (value) {
        case 1: return A_ALLOCATING;
        case 2: return A_ALLOCATION_SUCCEEDED;
        case 3: return A_ALLOCATION_FAILED;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<AllocationStatusProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<AllocationStatusProto>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<AllocationStatusProto>() {
            public AllocationStatusProto findValueByNumber(int number) {
              return AllocationStatusProto.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.getDescriptor().getEnumTypes().get(0);
    }

    private static final AllocationStatusProto[] VALUES = values();

    public static AllocationStatusProto valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private AllocationStatusProto(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:AllocationStatusProto)
  }

  /**
   * Protobuf enum {@code FinalJobStatusProto}
   */
  public enum FinalJobStatusProto
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>JOB_SUCCEEDED = 1;</code>
     */
    JOB_SUCCEEDED(0, 1),
    /**
     * <code>JOB_FAILED = 2;</code>
     */
    JOB_FAILED(1, 2),
    /**
     * <code>JOB_UNKOWN = 3;</code>
     */
    JOB_UNKOWN(2, 3),
    ;

    /**
     * <code>JOB_SUCCEEDED = 1;</code>
     */
    public static final int JOB_SUCCEEDED_VALUE = 1;
    /**
     * <code>JOB_FAILED = 2;</code>
     */
    public static final int JOB_FAILED_VALUE = 2;
    /**
     * <code>JOB_UNKOWN = 3;</code>
     */
    public static final int JOB_UNKOWN_VALUE = 3;


    public final int getNumber() { return value; }

    public static FinalJobStatusProto valueOf(int value) {
      switch (value) {
        case 1: return JOB_SUCCEEDED;
        case 2: return JOB_FAILED;
        case 3: return JOB_UNKOWN;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<FinalJobStatusProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<FinalJobStatusProto>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<FinalJobStatusProto>() {
            public FinalJobStatusProto findValueByNumber(int number) {
              return FinalJobStatusProto.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.getDescriptor().getEnumTypes().get(1);
    }

    private static final FinalJobStatusProto[] VALUES = values();

    public static FinalJobStatusProto valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private FinalJobStatusProto(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:FinalJobStatusProto)
  }

  public interface JobIdProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required int64 timestamp = 1;
    /**
     * <code>required int64 timestamp = 1;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>required int64 timestamp = 1;</code>
     */
    long getTimestamp();

    // required int32 id = 2;
    /**
     * <code>required int32 id = 2;</code>
     */
    boolean hasId();
    /**
     * <code>required int32 id = 2;</code>
     */
    int getId();
  }
  /**
   * Protobuf type {@code JobIdProto}
   */
  public static final class JobIdProto extends
      com.google.protobuf.GeneratedMessage
      implements JobIdProtoOrBuilder {
    // Use JobIdProto.newBuilder() to construct.
    private JobIdProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private JobIdProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final JobIdProto defaultInstance;
    public static JobIdProto getDefaultInstance() {
      return defaultInstance;
    }

    public JobIdProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private JobIdProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              timestamp_ = input.readInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              id_ = input.readInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_JobIdProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_JobIdProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder.class);
    }

    public static com.google.protobuf.Parser<JobIdProto> PARSER =
        new com.google.protobuf.AbstractParser<JobIdProto>() {
      public JobIdProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new JobIdProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<JobIdProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required int64 timestamp = 1;
    public static final int TIMESTAMP_FIELD_NUMBER = 1;
    private long timestamp_;
    /**
     * <code>required int64 timestamp = 1;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required int64 timestamp = 1;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    // required int32 id = 2;
    public static final int ID_FIELD_NUMBER = 2;
    private int id_;
    /**
     * <code>required int32 id = 2;</code>
     */
    public boolean hasId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required int32 id = 2;</code>
     */
    public int getId() {
      return id_;
    }

    private void initFields() {
      timestamp_ = 0L;
      id_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasTimestamp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt64(1, timestamp_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, id_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, timestamp_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, id_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto) obj;

      boolean result = true;
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result && (hasId() == other.hasId());
      if (hasId()) {
        result = result && (getId()
            == other.getId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code JobIdProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_JobIdProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_JobIdProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_JobIdProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.timestamp_ = timestamp_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.id_ = id_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance()) return this;
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        if (other.hasId()) {
          setId(other.getId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasTimestamp()) {
          
          return false;
        }
        if (!hasId()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required int64 timestamp = 1;
      private long timestamp_ ;
      /**
       * <code>required int64 timestamp = 1;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required int64 timestamp = 1;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>required int64 timestamp = 1;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000001;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 timestamp = 1;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000001);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // required int32 id = 2;
      private int id_ ;
      /**
       * <code>required int32 id = 2;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required int32 id = 2;</code>
       */
      public int getId() {
        return id_;
      }
      /**
       * <code>required int32 id = 2;</code>
       */
      public Builder setId(int value) {
        bitField0_ |= 0x00000002;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int32 id = 2;</code>
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        id_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:JobIdProto)
    }

    static {
      defaultInstance = new JobIdProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:JobIdProto)
  }

  public interface ResourceIdProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required int32 id = 1;
    /**
     * <code>required int32 id = 1;</code>
     */
    boolean hasId();
    /**
     * <code>required int32 id = 1;</code>
     */
    int getId();
  }
  /**
   * Protobuf type {@code ResourceIdProto}
   */
  public static final class ResourceIdProto extends
      com.google.protobuf.GeneratedMessage
      implements ResourceIdProtoOrBuilder {
    // Use ResourceIdProto.newBuilder() to construct.
    private ResourceIdProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ResourceIdProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ResourceIdProto defaultInstance;
    public static ResourceIdProto getDefaultInstance() {
      return defaultInstance;
    }

    public ResourceIdProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ResourceIdProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              id_ = input.readInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceIdProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceIdProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ResourceIdProto> PARSER =
        new com.google.protobuf.AbstractParser<ResourceIdProto>() {
      public ResourceIdProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ResourceIdProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ResourceIdProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required int32 id = 1;
    public static final int ID_FIELD_NUMBER = 1;
    private int id_;
    /**
     * <code>required int32 id = 1;</code>
     */
    public boolean hasId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required int32 id = 1;</code>
     */
    public int getId() {
      return id_;
    }

    private void initFields() {
      id_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt32(1, id_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, id_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto) obj;

      boolean result = true;
      result = result && (hasId() == other.hasId());
      if (hasId()) {
        result = result && (getId()
            == other.getId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code ResourceIdProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceIdProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceIdProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        id_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceIdProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.id_ = id_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance()) return this;
        if (other.hasId()) {
          setId(other.getId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasId()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required int32 id = 1;
      private int id_ ;
      /**
       * <code>required int32 id = 1;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required int32 id = 1;</code>
       */
      public int getId() {
        return id_;
      }
      /**
       * <code>required int32 id = 1;</code>
       */
      public Builder setId(int value) {
        bitField0_ |= 0x00000001;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int32 id = 1;</code>
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:ResourceIdProto)
    }

    static {
      defaultInstance = new ResourceIdProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:ResourceIdProto)
  }

  public interface AllocatedResourceProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string hostname = 1;
    /**
     * <code>required string hostname = 1;</code>
     */
    boolean hasHostname();
    /**
     * <code>required string hostname = 1;</code>
     */
    java.lang.String getHostname();
    /**
     * <code>required string hostname = 1;</code>
     */
    com.google.protobuf.ByteString
        getHostnameBytes();

    // required .ResourceIdProto resourceId = 2;
    /**
     * <code>required .ResourceIdProto resourceId = 2;</code>
     */
    boolean hasResourceId();
    /**
     * <code>required .ResourceIdProto resourceId = 2;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto getResourceId();
    /**
     * <code>required .ResourceIdProto resourceId = 2;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder getResourceIdOrBuilder();
  }
  /**
   * Protobuf type {@code AllocatedResourceProto}
   */
  public static final class AllocatedResourceProto extends
      com.google.protobuf.GeneratedMessage
      implements AllocatedResourceProtoOrBuilder {
    // Use AllocatedResourceProto.newBuilder() to construct.
    private AllocatedResourceProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AllocatedResourceProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AllocatedResourceProto defaultInstance;
    public static AllocatedResourceProto getDefaultInstance() {
      return defaultInstance;
    }

    public AllocatedResourceProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AllocatedResourceProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              hostname_ = input.readBytes();
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = resourceId_.toBuilder();
              }
              resourceId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(resourceId_);
                resourceId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocatedResourceProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocatedResourceProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AllocatedResourceProto> PARSER =
        new com.google.protobuf.AbstractParser<AllocatedResourceProto>() {
      public AllocatedResourceProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllocatedResourceProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AllocatedResourceProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string hostname = 1;
    public static final int HOSTNAME_FIELD_NUMBER = 1;
    private java.lang.Object hostname_;
    /**
     * <code>required string hostname = 1;</code>
     */
    public boolean hasHostname() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string hostname = 1;</code>
     */
    public java.lang.String getHostname() {
      java.lang.Object ref = hostname_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          hostname_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string hostname = 1;</code>
     */
    public com.google.protobuf.ByteString
        getHostnameBytes() {
      java.lang.Object ref = hostname_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        hostname_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required .ResourceIdProto resourceId = 2;
    public static final int RESOURCEID_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto resourceId_;
    /**
     * <code>required .ResourceIdProto resourceId = 2;</code>
     */
    public boolean hasResourceId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .ResourceIdProto resourceId = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto getResourceId() {
      return resourceId_;
    }
    /**
     * <code>required .ResourceIdProto resourceId = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder getResourceIdOrBuilder() {
      return resourceId_;
    }

    private void initFields() {
      hostname_ = "";
      resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasHostname()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasResourceId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getResourceId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getHostnameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, resourceId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getHostnameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, resourceId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto) obj;

      boolean result = true;
      result = result && (hasHostname() == other.hasHostname());
      if (hasHostname()) {
        result = result && getHostname()
            .equals(other.getHostname());
      }
      result = result && (hasResourceId() == other.hasResourceId());
      if (hasResourceId()) {
        result = result && getResourceId()
            .equals(other.getResourceId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasHostname()) {
        hash = (37 * hash) + HOSTNAME_FIELD_NUMBER;
        hash = (53 * hash) + getHostname().hashCode();
      }
      if (hasResourceId()) {
        hash = (37 * hash) + RESOURCEID_FIELD_NUMBER;
        hash = (53 * hash) + getResourceId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code AllocatedResourceProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocatedResourceProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocatedResourceProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getResourceIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        hostname_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (resourceIdBuilder_ == null) {
          resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
        } else {
          resourceIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocatedResourceProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.hostname_ = hostname_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (resourceIdBuilder_ == null) {
          result.resourceId_ = resourceId_;
        } else {
          result.resourceId_ = resourceIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.getDefaultInstance()) return this;
        if (other.hasHostname()) {
          bitField0_ |= 0x00000001;
          hostname_ = other.hostname_;
          onChanged();
        }
        if (other.hasResourceId()) {
          mergeResourceId(other.getResourceId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasHostname()) {
          
          return false;
        }
        if (!hasResourceId()) {
          
          return false;
        }
        if (!getResourceId().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string hostname = 1;
      private java.lang.Object hostname_ = "";
      /**
       * <code>required string hostname = 1;</code>
       */
      public boolean hasHostname() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string hostname = 1;</code>
       */
      public java.lang.String getHostname() {
        java.lang.Object ref = hostname_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          hostname_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string hostname = 1;</code>
       */
      public com.google.protobuf.ByteString
          getHostnameBytes() {
        java.lang.Object ref = hostname_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          hostname_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string hostname = 1;</code>
       */
      public Builder setHostname(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        hostname_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string hostname = 1;</code>
       */
      public Builder clearHostname() {
        bitField0_ = (bitField0_ & ~0x00000001);
        hostname_ = getDefaultInstance().getHostname();
        onChanged();
        return this;
      }
      /**
       * <code>required string hostname = 1;</code>
       */
      public Builder setHostnameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        hostname_ = value;
        onChanged();
        return this;
      }

      // required .ResourceIdProto resourceId = 2;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder> resourceIdBuilder_;
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public boolean hasResourceId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto getResourceId() {
        if (resourceIdBuilder_ == null) {
          return resourceId_;
        } else {
          return resourceIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public Builder setResourceId(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto value) {
        if (resourceIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          resourceId_ = value;
          onChanged();
        } else {
          resourceIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public Builder setResourceId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder builderForValue) {
        if (resourceIdBuilder_ == null) {
          resourceId_ = builderForValue.build();
          onChanged();
        } else {
          resourceIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public Builder mergeResourceId(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto value) {
        if (resourceIdBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              resourceId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance()) {
            resourceId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.newBuilder(resourceId_).mergeFrom(value).buildPartial();
          } else {
            resourceId_ = value;
          }
          onChanged();
        } else {
          resourceIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public Builder clearResourceId() {
        if (resourceIdBuilder_ == null) {
          resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
          onChanged();
        } else {
          resourceIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder getResourceIdBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getResourceIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder getResourceIdOrBuilder() {
        if (resourceIdBuilder_ != null) {
          return resourceIdBuilder_.getMessageOrBuilder();
        } else {
          return resourceId_;
        }
      }
      /**
       * <code>required .ResourceIdProto resourceId = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder> 
          getResourceIdFieldBuilder() {
        if (resourceIdBuilder_ == null) {
          resourceIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder>(
                  resourceId_,
                  getParentForChildren(),
                  isClean());
          resourceId_ = null;
        }
        return resourceIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:AllocatedResourceProto)
    }

    static {
      defaultInstance = new AllocatedResourceProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:AllocatedResourceProto)
  }

  public interface MpiJobAllocationUpdateProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string mpiJobName = 1;
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    boolean hasMpiJobName();
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    java.lang.String getMpiJobName();
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    com.google.protobuf.ByteString
        getMpiJobNameBytes();

    // repeated .AllocatedResourceProto newlyAllocatedResources = 2;
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto> 
        getNewlyAllocatedResourcesList();
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto getNewlyAllocatedResources(int index);
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    int getNewlyAllocatedResourcesCount();
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder> 
        getNewlyAllocatedResourcesOrBuilderList();
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder getNewlyAllocatedResourcesOrBuilder(
        int index);

    // required .AllocationStatusProto allocationStatus = 3;
    /**
     * <code>required .AllocationStatusProto allocationStatus = 3;</code>
     */
    boolean hasAllocationStatus();
    /**
     * <code>required .AllocationStatusProto allocationStatus = 3;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto getAllocationStatus();
  }
  /**
   * Protobuf type {@code MpiJobAllocationUpdateProto}
   */
  public static final class MpiJobAllocationUpdateProto extends
      com.google.protobuf.GeneratedMessage
      implements MpiJobAllocationUpdateProtoOrBuilder {
    // Use MpiJobAllocationUpdateProto.newBuilder() to construct.
    private MpiJobAllocationUpdateProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private MpiJobAllocationUpdateProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final MpiJobAllocationUpdateProto defaultInstance;
    public static MpiJobAllocationUpdateProto getDefaultInstance() {
      return defaultInstance;
    }

    public MpiJobAllocationUpdateProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private MpiJobAllocationUpdateProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              mpiJobName_ = input.readBytes();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                newlyAllocatedResources_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              newlyAllocatedResources_.add(input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.PARSER, extensionRegistry));
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto value = org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                allocationStatus_ = value;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          newlyAllocatedResources_ = java.util.Collections.unmodifiableList(newlyAllocatedResources_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_MpiJobAllocationUpdateProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_MpiJobAllocationUpdateProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder.class);
    }

    public static com.google.protobuf.Parser<MpiJobAllocationUpdateProto> PARSER =
        new com.google.protobuf.AbstractParser<MpiJobAllocationUpdateProto>() {
      public MpiJobAllocationUpdateProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new MpiJobAllocationUpdateProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<MpiJobAllocationUpdateProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string mpiJobName = 1;
    public static final int MPIJOBNAME_FIELD_NUMBER = 1;
    private java.lang.Object mpiJobName_;
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    public boolean hasMpiJobName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    public java.lang.String getMpiJobName() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          mpiJobName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    public com.google.protobuf.ByteString
        getMpiJobNameBytes() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mpiJobName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated .AllocatedResourceProto newlyAllocatedResources = 2;
    public static final int NEWLYALLOCATEDRESOURCES_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto> newlyAllocatedResources_;
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto> getNewlyAllocatedResourcesList() {
      return newlyAllocatedResources_;
    }
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder> 
        getNewlyAllocatedResourcesOrBuilderList() {
      return newlyAllocatedResources_;
    }
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    public int getNewlyAllocatedResourcesCount() {
      return newlyAllocatedResources_.size();
    }
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto getNewlyAllocatedResources(int index) {
      return newlyAllocatedResources_.get(index);
    }
    /**
     * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder getNewlyAllocatedResourcesOrBuilder(
        int index) {
      return newlyAllocatedResources_.get(index);
    }

    // required .AllocationStatusProto allocationStatus = 3;
    public static final int ALLOCATIONSTATUS_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto allocationStatus_;
    /**
     * <code>required .AllocationStatusProto allocationStatus = 3;</code>
     */
    public boolean hasAllocationStatus() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .AllocationStatusProto allocationStatus = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto getAllocationStatus() {
      return allocationStatus_;
    }

    private void initFields() {
      mpiJobName_ = "";
      newlyAllocatedResources_ = java.util.Collections.emptyList();
      allocationStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto.A_ALLOCATING;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasMpiJobName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasAllocationStatus()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getNewlyAllocatedResourcesCount(); i++) {
        if (!getNewlyAllocatedResources(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getMpiJobNameBytes());
      }
      for (int i = 0; i < newlyAllocatedResources_.size(); i++) {
        output.writeMessage(2, newlyAllocatedResources_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(3, allocationStatus_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getMpiJobNameBytes());
      }
      for (int i = 0; i < newlyAllocatedResources_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, newlyAllocatedResources_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, allocationStatus_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto) obj;

      boolean result = true;
      result = result && (hasMpiJobName() == other.hasMpiJobName());
      if (hasMpiJobName()) {
        result = result && getMpiJobName()
            .equals(other.getMpiJobName());
      }
      result = result && getNewlyAllocatedResourcesList()
          .equals(other.getNewlyAllocatedResourcesList());
      result = result && (hasAllocationStatus() == other.hasAllocationStatus());
      if (hasAllocationStatus()) {
        result = result &&
            (getAllocationStatus() == other.getAllocationStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasMpiJobName()) {
        hash = (37 * hash) + MPIJOBNAME_FIELD_NUMBER;
        hash = (53 * hash) + getMpiJobName().hashCode();
      }
      if (getNewlyAllocatedResourcesCount() > 0) {
        hash = (37 * hash) + NEWLYALLOCATEDRESOURCES_FIELD_NUMBER;
        hash = (53 * hash) + getNewlyAllocatedResourcesList().hashCode();
      }
      if (hasAllocationStatus()) {
        hash = (37 * hash) + ALLOCATIONSTATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getAllocationStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code MpiJobAllocationUpdateProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_MpiJobAllocationUpdateProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_MpiJobAllocationUpdateProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNewlyAllocatedResourcesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        mpiJobName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (newlyAllocatedResourcesBuilder_ == null) {
          newlyAllocatedResources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          newlyAllocatedResourcesBuilder_.clear();
        }
        allocationStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto.A_ALLOCATING;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_MpiJobAllocationUpdateProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.mpiJobName_ = mpiJobName_;
        if (newlyAllocatedResourcesBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            newlyAllocatedResources_ = java.util.Collections.unmodifiableList(newlyAllocatedResources_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.newlyAllocatedResources_ = newlyAllocatedResources_;
        } else {
          result.newlyAllocatedResources_ = newlyAllocatedResourcesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000002;
        }
        result.allocationStatus_ = allocationStatus_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.getDefaultInstance()) return this;
        if (other.hasMpiJobName()) {
          bitField0_ |= 0x00000001;
          mpiJobName_ = other.mpiJobName_;
          onChanged();
        }
        if (newlyAllocatedResourcesBuilder_ == null) {
          if (!other.newlyAllocatedResources_.isEmpty()) {
            if (newlyAllocatedResources_.isEmpty()) {
              newlyAllocatedResources_ = other.newlyAllocatedResources_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureNewlyAllocatedResourcesIsMutable();
              newlyAllocatedResources_.addAll(other.newlyAllocatedResources_);
            }
            onChanged();
          }
        } else {
          if (!other.newlyAllocatedResources_.isEmpty()) {
            if (newlyAllocatedResourcesBuilder_.isEmpty()) {
              newlyAllocatedResourcesBuilder_.dispose();
              newlyAllocatedResourcesBuilder_ = null;
              newlyAllocatedResources_ = other.newlyAllocatedResources_;
              bitField0_ = (bitField0_ & ~0x00000002);
              newlyAllocatedResourcesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNewlyAllocatedResourcesFieldBuilder() : null;
            } else {
              newlyAllocatedResourcesBuilder_.addAllMessages(other.newlyAllocatedResources_);
            }
          }
        }
        if (other.hasAllocationStatus()) {
          setAllocationStatus(other.getAllocationStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasMpiJobName()) {
          
          return false;
        }
        if (!hasAllocationStatus()) {
          
          return false;
        }
        for (int i = 0; i < getNewlyAllocatedResourcesCount(); i++) {
          if (!getNewlyAllocatedResources(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string mpiJobName = 1;
      private java.lang.Object mpiJobName_ = "";
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public boolean hasMpiJobName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public java.lang.String getMpiJobName() {
        java.lang.Object ref = mpiJobName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          mpiJobName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public com.google.protobuf.ByteString
          getMpiJobNameBytes() {
        java.lang.Object ref = mpiJobName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mpiJobName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public Builder setMpiJobName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        mpiJobName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public Builder clearMpiJobName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        mpiJobName_ = getDefaultInstance().getMpiJobName();
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public Builder setMpiJobNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        mpiJobName_ = value;
        onChanged();
        return this;
      }

      // repeated .AllocatedResourceProto newlyAllocatedResources = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto> newlyAllocatedResources_ =
        java.util.Collections.emptyList();
      private void ensureNewlyAllocatedResourcesIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          newlyAllocatedResources_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto>(newlyAllocatedResources_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder> newlyAllocatedResourcesBuilder_;

      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto> getNewlyAllocatedResourcesList() {
        if (newlyAllocatedResourcesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(newlyAllocatedResources_);
        } else {
          return newlyAllocatedResourcesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public int getNewlyAllocatedResourcesCount() {
        if (newlyAllocatedResourcesBuilder_ == null) {
          return newlyAllocatedResources_.size();
        } else {
          return newlyAllocatedResourcesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto getNewlyAllocatedResources(int index) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          return newlyAllocatedResources_.get(index);
        } else {
          return newlyAllocatedResourcesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder setNewlyAllocatedResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto value) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNewlyAllocatedResourcesIsMutable();
          newlyAllocatedResources_.set(index, value);
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder setNewlyAllocatedResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder builderForValue) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          ensureNewlyAllocatedResourcesIsMutable();
          newlyAllocatedResources_.set(index, builderForValue.build());
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder addNewlyAllocatedResources(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto value) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNewlyAllocatedResourcesIsMutable();
          newlyAllocatedResources_.add(value);
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder addNewlyAllocatedResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto value) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNewlyAllocatedResourcesIsMutable();
          newlyAllocatedResources_.add(index, value);
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder addNewlyAllocatedResources(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder builderForValue) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          ensureNewlyAllocatedResourcesIsMutable();
          newlyAllocatedResources_.add(builderForValue.build());
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder addNewlyAllocatedResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder builderForValue) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          ensureNewlyAllocatedResourcesIsMutable();
          newlyAllocatedResources_.add(index, builderForValue.build());
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder addAllNewlyAllocatedResources(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto> values) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          ensureNewlyAllocatedResourcesIsMutable();
          super.addAll(values, newlyAllocatedResources_);
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder clearNewlyAllocatedResources() {
        if (newlyAllocatedResourcesBuilder_ == null) {
          newlyAllocatedResources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public Builder removeNewlyAllocatedResources(int index) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          ensureNewlyAllocatedResourcesIsMutable();
          newlyAllocatedResources_.remove(index);
          onChanged();
        } else {
          newlyAllocatedResourcesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder getNewlyAllocatedResourcesBuilder(
          int index) {
        return getNewlyAllocatedResourcesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder getNewlyAllocatedResourcesOrBuilder(
          int index) {
        if (newlyAllocatedResourcesBuilder_ == null) {
          return newlyAllocatedResources_.get(index);  } else {
          return newlyAllocatedResourcesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder> 
           getNewlyAllocatedResourcesOrBuilderList() {
        if (newlyAllocatedResourcesBuilder_ != null) {
          return newlyAllocatedResourcesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(newlyAllocatedResources_);
        }
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder addNewlyAllocatedResourcesBuilder() {
        return getNewlyAllocatedResourcesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.getDefaultInstance());
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder addNewlyAllocatedResourcesBuilder(
          int index) {
        return getNewlyAllocatedResourcesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.getDefaultInstance());
      }
      /**
       * <code>repeated .AllocatedResourceProto newlyAllocatedResources = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder> 
           getNewlyAllocatedResourcesBuilderList() {
        return getNewlyAllocatedResourcesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder> 
          getNewlyAllocatedResourcesFieldBuilder() {
        if (newlyAllocatedResourcesBuilder_ == null) {
          newlyAllocatedResourcesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocatedResourceProtoOrBuilder>(
                  newlyAllocatedResources_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          newlyAllocatedResources_ = null;
        }
        return newlyAllocatedResourcesBuilder_;
      }

      // required .AllocationStatusProto allocationStatus = 3;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto allocationStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto.A_ALLOCATING;
      /**
       * <code>required .AllocationStatusProto allocationStatus = 3;</code>
       */
      public boolean hasAllocationStatus() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required .AllocationStatusProto allocationStatus = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto getAllocationStatus() {
        return allocationStatus_;
      }
      /**
       * <code>required .AllocationStatusProto allocationStatus = 3;</code>
       */
      public Builder setAllocationStatus(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000004;
        allocationStatus_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .AllocationStatusProto allocationStatus = 3;</code>
       */
      public Builder clearAllocationStatus() {
        bitField0_ = (bitField0_ & ~0x00000004);
        allocationStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocationStatusProto.A_ALLOCATING;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:MpiJobAllocationUpdateProto)
    }

    static {
      defaultInstance = new MpiJobAllocationUpdateProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:MpiJobAllocationUpdateProto)
  }

  public interface StringStringMapProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string key = 1;
    /**
     * <code>optional string key = 1;</code>
     */
    boolean hasKey();
    /**
     * <code>optional string key = 1;</code>
     */
    java.lang.String getKey();
    /**
     * <code>optional string key = 1;</code>
     */
    com.google.protobuf.ByteString
        getKeyBytes();

    // optional string value = 2;
    /**
     * <code>optional string value = 2;</code>
     */
    boolean hasValue();
    /**
     * <code>optional string value = 2;</code>
     */
    java.lang.String getValue();
    /**
     * <code>optional string value = 2;</code>
     */
    com.google.protobuf.ByteString
        getValueBytes();
  }
  /**
   * Protobuf type {@code StringStringMapProto}
   */
  public static final class StringStringMapProto extends
      com.google.protobuf.GeneratedMessage
      implements StringStringMapProtoOrBuilder {
    // Use StringStringMapProto.newBuilder() to construct.
    private StringStringMapProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StringStringMapProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StringStringMapProto defaultInstance;
    public static StringStringMapProto getDefaultInstance() {
      return defaultInstance;
    }

    public StringStringMapProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StringStringMapProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              key_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              value_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_StringStringMapProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_StringStringMapProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StringStringMapProto> PARSER =
        new com.google.protobuf.AbstractParser<StringStringMapProto>() {
      public StringStringMapProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StringStringMapProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StringStringMapProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string key = 1;
    public static final int KEY_FIELD_NUMBER = 1;
    private java.lang.Object key_;
    /**
     * <code>optional string key = 1;</code>
     */
    public boolean hasKey() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string key = 1;</code>
     */
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          key_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string key = 1;</code>
     */
    public com.google.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string value = 2;
    public static final int VALUE_FIELD_NUMBER = 2;
    private java.lang.Object value_;
    /**
     * <code>optional string value = 2;</code>
     */
    public boolean hasValue() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string value = 2;</code>
     */
    public java.lang.String getValue() {
      java.lang.Object ref = value_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          value_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string value = 2;</code>
     */
    public com.google.protobuf.ByteString
        getValueBytes() {
      java.lang.Object ref = value_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        value_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      key_ = "";
      value_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getKeyBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getValueBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getKeyBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getValueBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto) obj;

      boolean result = true;
      result = result && (hasKey() == other.hasKey());
      if (hasKey()) {
        result = result && getKey()
            .equals(other.getKey());
      }
      result = result && (hasValue() == other.hasValue());
      if (hasValue()) {
        result = result && getValue()
            .equals(other.getValue());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasKey()) {
        hash = (37 * hash) + KEY_FIELD_NUMBER;
        hash = (53 * hash) + getKey().hashCode();
      }
      if (hasValue()) {
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + getValue().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code StringStringMapProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_StringStringMapProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_StringStringMapProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        key_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        value_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_StringStringMapProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.key_ = key_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.value_ = value_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.getDefaultInstance()) return this;
        if (other.hasKey()) {
          bitField0_ |= 0x00000001;
          key_ = other.key_;
          onChanged();
        }
        if (other.hasValue()) {
          bitField0_ |= 0x00000002;
          value_ = other.value_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string key = 1;
      private java.lang.Object key_ = "";
      /**
       * <code>optional string key = 1;</code>
       */
      public boolean hasKey() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          key_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public com.google.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder clearKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder setKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }

      // optional string value = 2;
      private java.lang.Object value_ = "";
      /**
       * <code>optional string value = 2;</code>
       */
      public boolean hasValue() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string value = 2;</code>
       */
      public java.lang.String getValue() {
        java.lang.Object ref = value_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          value_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string value = 2;</code>
       */
      public com.google.protobuf.ByteString
          getValueBytes() {
        java.lang.Object ref = value_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          value_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string value = 2;</code>
       */
      public Builder setValue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        value_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string value = 2;</code>
       */
      public Builder clearValue() {
        bitField0_ = (bitField0_ & ~0x00000002);
        value_ = getDefaultInstance().getValue();
        onChanged();
        return this;
      }
      /**
       * <code>optional string value = 2;</code>
       */
      public Builder setValueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        value_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:StringStringMapProto)
    }

    static {
      defaultInstance = new StringStringMapProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:StringStringMapProto)
  }

  public interface LocalResourceProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string url = 1;
    /**
     * <code>required string url = 1;</code>
     */
    boolean hasUrl();
    /**
     * <code>required string url = 1;</code>
     */
    java.lang.String getUrl();
    /**
     * <code>required string url = 1;</code>
     */
    com.google.protobuf.ByteString
        getUrlBytes();

    // required string localizedName = 2;
    /**
     * <code>required string localizedName = 2;</code>
     */
    boolean hasLocalizedName();
    /**
     * <code>required string localizedName = 2;</code>
     */
    java.lang.String getLocalizedName();
    /**
     * <code>required string localizedName = 2;</code>
     */
    com.google.protobuf.ByteString
        getLocalizedNameBytes();

    // required int64 size = 3;
    /**
     * <code>required int64 size = 3;</code>
     */
    boolean hasSize();
    /**
     * <code>required int64 size = 3;</code>
     */
    long getSize();

    // required int64 timestamp = 4;
    /**
     * <code>required int64 timestamp = 4;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>required int64 timestamp = 4;</code>
     */
    long getTimestamp();
  }
  /**
   * Protobuf type {@code LocalResourceProto}
   */
  public static final class LocalResourceProto extends
      com.google.protobuf.GeneratedMessage
      implements LocalResourceProtoOrBuilder {
    // Use LocalResourceProto.newBuilder() to construct.
    private LocalResourceProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private LocalResourceProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final LocalResourceProto defaultInstance;
    public static LocalResourceProto getDefaultInstance() {
      return defaultInstance;
    }

    public LocalResourceProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private LocalResourceProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              url_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              localizedName_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              size_ = input.readInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              timestamp_ = input.readInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LocalResourceProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LocalResourceProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder.class);
    }

    public static com.google.protobuf.Parser<LocalResourceProto> PARSER =
        new com.google.protobuf.AbstractParser<LocalResourceProto>() {
      public LocalResourceProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new LocalResourceProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<LocalResourceProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string url = 1;
    public static final int URL_FIELD_NUMBER = 1;
    private java.lang.Object url_;
    /**
     * <code>required string url = 1;</code>
     */
    public boolean hasUrl() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string url = 1;</code>
     */
    public java.lang.String getUrl() {
      java.lang.Object ref = url_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          url_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string url = 1;</code>
     */
    public com.google.protobuf.ByteString
        getUrlBytes() {
      java.lang.Object ref = url_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        url_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required string localizedName = 2;
    public static final int LOCALIZEDNAME_FIELD_NUMBER = 2;
    private java.lang.Object localizedName_;
    /**
     * <code>required string localizedName = 2;</code>
     */
    public boolean hasLocalizedName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required string localizedName = 2;</code>
     */
    public java.lang.String getLocalizedName() {
      java.lang.Object ref = localizedName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          localizedName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string localizedName = 2;</code>
     */
    public com.google.protobuf.ByteString
        getLocalizedNameBytes() {
      java.lang.Object ref = localizedName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        localizedName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required int64 size = 3;
    public static final int SIZE_FIELD_NUMBER = 3;
    private long size_;
    /**
     * <code>required int64 size = 3;</code>
     */
    public boolean hasSize() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required int64 size = 3;</code>
     */
    public long getSize() {
      return size_;
    }

    // required int64 timestamp = 4;
    public static final int TIMESTAMP_FIELD_NUMBER = 4;
    private long timestamp_;
    /**
     * <code>required int64 timestamp = 4;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required int64 timestamp = 4;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    private void initFields() {
      url_ = "";
      localizedName_ = "";
      size_ = 0L;
      timestamp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasUrl()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLocalizedName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSize()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTimestamp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getUrlBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getLocalizedNameBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(3, size_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt64(4, timestamp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getUrlBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getLocalizedNameBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, size_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, timestamp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto) obj;

      boolean result = true;
      result = result && (hasUrl() == other.hasUrl());
      if (hasUrl()) {
        result = result && getUrl()
            .equals(other.getUrl());
      }
      result = result && (hasLocalizedName() == other.hasLocalizedName());
      if (hasLocalizedName()) {
        result = result && getLocalizedName()
            .equals(other.getLocalizedName());
      }
      result = result && (hasSize() == other.hasSize());
      if (hasSize()) {
        result = result && (getSize()
            == other.getSize());
      }
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasUrl()) {
        hash = (37 * hash) + URL_FIELD_NUMBER;
        hash = (53 * hash) + getUrl().hashCode();
      }
      if (hasLocalizedName()) {
        hash = (37 * hash) + LOCALIZEDNAME_FIELD_NUMBER;
        hash = (53 * hash) + getLocalizedName().hashCode();
      }
      if (hasSize()) {
        hash = (37 * hash) + SIZE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getSize());
      }
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code LocalResourceProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LocalResourceProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LocalResourceProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        url_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        localizedName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        size_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LocalResourceProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.url_ = url_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.localizedName_ = localizedName_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.size_ = size_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.timestamp_ = timestamp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.getDefaultInstance()) return this;
        if (other.hasUrl()) {
          bitField0_ |= 0x00000001;
          url_ = other.url_;
          onChanged();
        }
        if (other.hasLocalizedName()) {
          bitField0_ |= 0x00000002;
          localizedName_ = other.localizedName_;
          onChanged();
        }
        if (other.hasSize()) {
          setSize(other.getSize());
        }
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasUrl()) {
          
          return false;
        }
        if (!hasLocalizedName()) {
          
          return false;
        }
        if (!hasSize()) {
          
          return false;
        }
        if (!hasTimestamp()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string url = 1;
      private java.lang.Object url_ = "";
      /**
       * <code>required string url = 1;</code>
       */
      public boolean hasUrl() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string url = 1;</code>
       */
      public java.lang.String getUrl() {
        java.lang.Object ref = url_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          url_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string url = 1;</code>
       */
      public com.google.protobuf.ByteString
          getUrlBytes() {
        java.lang.Object ref = url_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          url_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string url = 1;</code>
       */
      public Builder setUrl(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        url_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string url = 1;</code>
       */
      public Builder clearUrl() {
        bitField0_ = (bitField0_ & ~0x00000001);
        url_ = getDefaultInstance().getUrl();
        onChanged();
        return this;
      }
      /**
       * <code>required string url = 1;</code>
       */
      public Builder setUrlBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        url_ = value;
        onChanged();
        return this;
      }

      // required string localizedName = 2;
      private java.lang.Object localizedName_ = "";
      /**
       * <code>required string localizedName = 2;</code>
       */
      public boolean hasLocalizedName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string localizedName = 2;</code>
       */
      public java.lang.String getLocalizedName() {
        java.lang.Object ref = localizedName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          localizedName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string localizedName = 2;</code>
       */
      public com.google.protobuf.ByteString
          getLocalizedNameBytes() {
        java.lang.Object ref = localizedName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          localizedName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string localizedName = 2;</code>
       */
      public Builder setLocalizedName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        localizedName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string localizedName = 2;</code>
       */
      public Builder clearLocalizedName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        localizedName_ = getDefaultInstance().getLocalizedName();
        onChanged();
        return this;
      }
      /**
       * <code>required string localizedName = 2;</code>
       */
      public Builder setLocalizedNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        localizedName_ = value;
        onChanged();
        return this;
      }

      // required int64 size = 3;
      private long size_ ;
      /**
       * <code>required int64 size = 3;</code>
       */
      public boolean hasSize() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required int64 size = 3;</code>
       */
      public long getSize() {
        return size_;
      }
      /**
       * <code>required int64 size = 3;</code>
       */
      public Builder setSize(long value) {
        bitField0_ |= 0x00000004;
        size_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 size = 3;</code>
       */
      public Builder clearSize() {
        bitField0_ = (bitField0_ & ~0x00000004);
        size_ = 0L;
        onChanged();
        return this;
      }

      // required int64 timestamp = 4;
      private long timestamp_ ;
      /**
       * <code>required int64 timestamp = 4;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required int64 timestamp = 4;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>required int64 timestamp = 4;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000008;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 timestamp = 4;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000008);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:LocalResourceProto)
    }

    static {
      defaultInstance = new LocalResourceProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:LocalResourceProto)
  }

  public interface ResourceLaunchContextProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .ResourceIdProto resourceId = 1;
    /**
     * <code>required .ResourceIdProto resourceId = 1;</code>
     */
    boolean hasResourceId();
    /**
     * <code>required .ResourceIdProto resourceId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto getResourceId();
    /**
     * <code>required .ResourceIdProto resourceId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder getResourceIdOrBuilder();

    // optional string commandline = 2;
    /**
     * <code>optional string commandline = 2;</code>
     */
    boolean hasCommandline();
    /**
     * <code>optional string commandline = 2;</code>
     */
    java.lang.String getCommandline();
    /**
     * <code>optional string commandline = 2;</code>
     */
    com.google.protobuf.ByteString
        getCommandlineBytes();

    // repeated .StringStringMapProto envs = 3;
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> 
        getEnvsList();
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto getEnvs(int index);
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    int getEnvsCount();
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
        getEnvsOrBuilderList();
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder getEnvsOrBuilder(
        int index);

    // repeated .LocalResourceProto localResources = 4;
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto> 
        getLocalResourcesList();
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto getLocalResources(int index);
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    int getLocalResourcesCount();
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder> 
        getLocalResourcesOrBuilderList();
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder getLocalResourcesOrBuilder(
        int index);

    // optional string mpiDaemonCommandline = 5;
    /**
     * <code>optional string mpiDaemonCommandline = 5;</code>
     */
    boolean hasMpiDaemonCommandline();
    /**
     * <code>optional string mpiDaemonCommandline = 5;</code>
     */
    java.lang.String getMpiDaemonCommandline();
    /**
     * <code>optional string mpiDaemonCommandline = 5;</code>
     */
    com.google.protobuf.ByteString
        getMpiDaemonCommandlineBytes();

    // repeated .StringStringMapProto mpiDaemonEnvs = 6;
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> 
        getMpiDaemonEnvsList();
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto getMpiDaemonEnvs(int index);
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    int getMpiDaemonEnvsCount();
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
        getMpiDaemonEnvsOrBuilderList();
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder getMpiDaemonEnvsOrBuilder(
        int index);

    // optional int32 localPeers = 7;
    /**
     * <code>optional int32 localPeers = 7;</code>
     */
    boolean hasLocalPeers();
    /**
     * <code>optional int32 localPeers = 7;</code>
     */
    int getLocalPeers();
  }
  /**
   * Protobuf type {@code ResourceLaunchContextProto}
   */
  public static final class ResourceLaunchContextProto extends
      com.google.protobuf.GeneratedMessage
      implements ResourceLaunchContextProtoOrBuilder {
    // Use ResourceLaunchContextProto.newBuilder() to construct.
    private ResourceLaunchContextProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ResourceLaunchContextProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ResourceLaunchContextProto defaultInstance;
    public static ResourceLaunchContextProto getDefaultInstance() {
      return defaultInstance;
    }

    public ResourceLaunchContextProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ResourceLaunchContextProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = resourceId_.toBuilder();
              }
              resourceId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(resourceId_);
                resourceId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              commandline_ = input.readBytes();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                envs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              envs_.add(input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.PARSER, extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                localResources_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto>();
                mutable_bitField0_ |= 0x00000008;
              }
              localResources_.add(input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.PARSER, extensionRegistry));
              break;
            }
            case 42: {
              bitField0_ |= 0x00000004;
              mpiDaemonCommandline_ = input.readBytes();
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                mpiDaemonEnvs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              mpiDaemonEnvs_.add(input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              bitField0_ |= 0x00000008;
              localPeers_ = input.readInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          envs_ = java.util.Collections.unmodifiableList(envs_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          localResources_ = java.util.Collections.unmodifiableList(localResources_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          mpiDaemonEnvs_ = java.util.Collections.unmodifiableList(mpiDaemonEnvs_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceLaunchContextProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceLaunchContextProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ResourceLaunchContextProto> PARSER =
        new com.google.protobuf.AbstractParser<ResourceLaunchContextProto>() {
      public ResourceLaunchContextProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ResourceLaunchContextProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ResourceLaunchContextProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .ResourceIdProto resourceId = 1;
    public static final int RESOURCEID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto resourceId_;
    /**
     * <code>required .ResourceIdProto resourceId = 1;</code>
     */
    public boolean hasResourceId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .ResourceIdProto resourceId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto getResourceId() {
      return resourceId_;
    }
    /**
     * <code>required .ResourceIdProto resourceId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder getResourceIdOrBuilder() {
      return resourceId_;
    }

    // optional string commandline = 2;
    public static final int COMMANDLINE_FIELD_NUMBER = 2;
    private java.lang.Object commandline_;
    /**
     * <code>optional string commandline = 2;</code>
     */
    public boolean hasCommandline() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string commandline = 2;</code>
     */
    public java.lang.String getCommandline() {
      java.lang.Object ref = commandline_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          commandline_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string commandline = 2;</code>
     */
    public com.google.protobuf.ByteString
        getCommandlineBytes() {
      java.lang.Object ref = commandline_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        commandline_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated .StringStringMapProto envs = 3;
    public static final int ENVS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> envs_;
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> getEnvsList() {
      return envs_;
    }
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
        getEnvsOrBuilderList() {
      return envs_;
    }
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    public int getEnvsCount() {
      return envs_.size();
    }
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto getEnvs(int index) {
      return envs_.get(index);
    }
    /**
     * <code>repeated .StringStringMapProto envs = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder getEnvsOrBuilder(
        int index) {
      return envs_.get(index);
    }

    // repeated .LocalResourceProto localResources = 4;
    public static final int LOCALRESOURCES_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto> localResources_;
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto> getLocalResourcesList() {
      return localResources_;
    }
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder> 
        getLocalResourcesOrBuilderList() {
      return localResources_;
    }
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    public int getLocalResourcesCount() {
      return localResources_.size();
    }
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto getLocalResources(int index) {
      return localResources_.get(index);
    }
    /**
     * <code>repeated .LocalResourceProto localResources = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder getLocalResourcesOrBuilder(
        int index) {
      return localResources_.get(index);
    }

    // optional string mpiDaemonCommandline = 5;
    public static final int MPIDAEMONCOMMANDLINE_FIELD_NUMBER = 5;
    private java.lang.Object mpiDaemonCommandline_;
    /**
     * <code>optional string mpiDaemonCommandline = 5;</code>
     */
    public boolean hasMpiDaemonCommandline() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string mpiDaemonCommandline = 5;</code>
     */
    public java.lang.String getMpiDaemonCommandline() {
      java.lang.Object ref = mpiDaemonCommandline_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          mpiDaemonCommandline_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string mpiDaemonCommandline = 5;</code>
     */
    public com.google.protobuf.ByteString
        getMpiDaemonCommandlineBytes() {
      java.lang.Object ref = mpiDaemonCommandline_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mpiDaemonCommandline_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated .StringStringMapProto mpiDaemonEnvs = 6;
    public static final int MPIDAEMONENVS_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> mpiDaemonEnvs_;
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> getMpiDaemonEnvsList() {
      return mpiDaemonEnvs_;
    }
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
        getMpiDaemonEnvsOrBuilderList() {
      return mpiDaemonEnvs_;
    }
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    public int getMpiDaemonEnvsCount() {
      return mpiDaemonEnvs_.size();
    }
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto getMpiDaemonEnvs(int index) {
      return mpiDaemonEnvs_.get(index);
    }
    /**
     * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder getMpiDaemonEnvsOrBuilder(
        int index) {
      return mpiDaemonEnvs_.get(index);
    }

    // optional int32 localPeers = 7;
    public static final int LOCALPEERS_FIELD_NUMBER = 7;
    private int localPeers_;
    /**
     * <code>optional int32 localPeers = 7;</code>
     */
    public boolean hasLocalPeers() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional int32 localPeers = 7;</code>
     */
    public int getLocalPeers() {
      return localPeers_;
    }

    private void initFields() {
      resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
      commandline_ = "";
      envs_ = java.util.Collections.emptyList();
      localResources_ = java.util.Collections.emptyList();
      mpiDaemonCommandline_ = "";
      mpiDaemonEnvs_ = java.util.Collections.emptyList();
      localPeers_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasResourceId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getResourceId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getLocalResourcesCount(); i++) {
        if (!getLocalResources(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, resourceId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getCommandlineBytes());
      }
      for (int i = 0; i < envs_.size(); i++) {
        output.writeMessage(3, envs_.get(i));
      }
      for (int i = 0; i < localResources_.size(); i++) {
        output.writeMessage(4, localResources_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(5, getMpiDaemonCommandlineBytes());
      }
      for (int i = 0; i < mpiDaemonEnvs_.size(); i++) {
        output.writeMessage(6, mpiDaemonEnvs_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt32(7, localPeers_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, resourceId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getCommandlineBytes());
      }
      for (int i = 0; i < envs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, envs_.get(i));
      }
      for (int i = 0; i < localResources_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, localResources_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(5, getMpiDaemonCommandlineBytes());
      }
      for (int i = 0; i < mpiDaemonEnvs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, mpiDaemonEnvs_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, localPeers_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto) obj;

      boolean result = true;
      result = result && (hasResourceId() == other.hasResourceId());
      if (hasResourceId()) {
        result = result && getResourceId()
            .equals(other.getResourceId());
      }
      result = result && (hasCommandline() == other.hasCommandline());
      if (hasCommandline()) {
        result = result && getCommandline()
            .equals(other.getCommandline());
      }
      result = result && getEnvsList()
          .equals(other.getEnvsList());
      result = result && getLocalResourcesList()
          .equals(other.getLocalResourcesList());
      result = result && (hasMpiDaemonCommandline() == other.hasMpiDaemonCommandline());
      if (hasMpiDaemonCommandline()) {
        result = result && getMpiDaemonCommandline()
            .equals(other.getMpiDaemonCommandline());
      }
      result = result && getMpiDaemonEnvsList()
          .equals(other.getMpiDaemonEnvsList());
      result = result && (hasLocalPeers() == other.hasLocalPeers());
      if (hasLocalPeers()) {
        result = result && (getLocalPeers()
            == other.getLocalPeers());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasResourceId()) {
        hash = (37 * hash) + RESOURCEID_FIELD_NUMBER;
        hash = (53 * hash) + getResourceId().hashCode();
      }
      if (hasCommandline()) {
        hash = (37 * hash) + COMMANDLINE_FIELD_NUMBER;
        hash = (53 * hash) + getCommandline().hashCode();
      }
      if (getEnvsCount() > 0) {
        hash = (37 * hash) + ENVS_FIELD_NUMBER;
        hash = (53 * hash) + getEnvsList().hashCode();
      }
      if (getLocalResourcesCount() > 0) {
        hash = (37 * hash) + LOCALRESOURCES_FIELD_NUMBER;
        hash = (53 * hash) + getLocalResourcesList().hashCode();
      }
      if (hasMpiDaemonCommandline()) {
        hash = (37 * hash) + MPIDAEMONCOMMANDLINE_FIELD_NUMBER;
        hash = (53 * hash) + getMpiDaemonCommandline().hashCode();
      }
      if (getMpiDaemonEnvsCount() > 0) {
        hash = (37 * hash) + MPIDAEMONENVS_FIELD_NUMBER;
        hash = (53 * hash) + getMpiDaemonEnvsList().hashCode();
      }
      if (hasLocalPeers()) {
        hash = (37 * hash) + LOCALPEERS_FIELD_NUMBER;
        hash = (53 * hash) + getLocalPeers();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code ResourceLaunchContextProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceLaunchContextProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceLaunchContextProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getResourceIdFieldBuilder();
          getEnvsFieldBuilder();
          getLocalResourcesFieldBuilder();
          getMpiDaemonEnvsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (resourceIdBuilder_ == null) {
          resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
        } else {
          resourceIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        commandline_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (envsBuilder_ == null) {
          envs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          envsBuilder_.clear();
        }
        if (localResourcesBuilder_ == null) {
          localResources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          localResourcesBuilder_.clear();
        }
        mpiDaemonCommandline_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        if (mpiDaemonEnvsBuilder_ == null) {
          mpiDaemonEnvs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          mpiDaemonEnvsBuilder_.clear();
        }
        localPeers_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_ResourceLaunchContextProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (resourceIdBuilder_ == null) {
          result.resourceId_ = resourceId_;
        } else {
          result.resourceId_ = resourceIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.commandline_ = commandline_;
        if (envsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            envs_ = java.util.Collections.unmodifiableList(envs_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.envs_ = envs_;
        } else {
          result.envs_ = envsBuilder_.build();
        }
        if (localResourcesBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008)) {
            localResources_ = java.util.Collections.unmodifiableList(localResources_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.localResources_ = localResources_;
        } else {
          result.localResources_ = localResourcesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000004;
        }
        result.mpiDaemonCommandline_ = mpiDaemonCommandline_;
        if (mpiDaemonEnvsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            mpiDaemonEnvs_ = java.util.Collections.unmodifiableList(mpiDaemonEnvs_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.mpiDaemonEnvs_ = mpiDaemonEnvs_;
        } else {
          result.mpiDaemonEnvs_ = mpiDaemonEnvsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000008;
        }
        result.localPeers_ = localPeers_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.getDefaultInstance()) return this;
        if (other.hasResourceId()) {
          mergeResourceId(other.getResourceId());
        }
        if (other.hasCommandline()) {
          bitField0_ |= 0x00000002;
          commandline_ = other.commandline_;
          onChanged();
        }
        if (envsBuilder_ == null) {
          if (!other.envs_.isEmpty()) {
            if (envs_.isEmpty()) {
              envs_ = other.envs_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureEnvsIsMutable();
              envs_.addAll(other.envs_);
            }
            onChanged();
          }
        } else {
          if (!other.envs_.isEmpty()) {
            if (envsBuilder_.isEmpty()) {
              envsBuilder_.dispose();
              envsBuilder_ = null;
              envs_ = other.envs_;
              bitField0_ = (bitField0_ & ~0x00000004);
              envsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getEnvsFieldBuilder() : null;
            } else {
              envsBuilder_.addAllMessages(other.envs_);
            }
          }
        }
        if (localResourcesBuilder_ == null) {
          if (!other.localResources_.isEmpty()) {
            if (localResources_.isEmpty()) {
              localResources_ = other.localResources_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureLocalResourcesIsMutable();
              localResources_.addAll(other.localResources_);
            }
            onChanged();
          }
        } else {
          if (!other.localResources_.isEmpty()) {
            if (localResourcesBuilder_.isEmpty()) {
              localResourcesBuilder_.dispose();
              localResourcesBuilder_ = null;
              localResources_ = other.localResources_;
              bitField0_ = (bitField0_ & ~0x00000008);
              localResourcesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getLocalResourcesFieldBuilder() : null;
            } else {
              localResourcesBuilder_.addAllMessages(other.localResources_);
            }
          }
        }
        if (other.hasMpiDaemonCommandline()) {
          bitField0_ |= 0x00000010;
          mpiDaemonCommandline_ = other.mpiDaemonCommandline_;
          onChanged();
        }
        if (mpiDaemonEnvsBuilder_ == null) {
          if (!other.mpiDaemonEnvs_.isEmpty()) {
            if (mpiDaemonEnvs_.isEmpty()) {
              mpiDaemonEnvs_ = other.mpiDaemonEnvs_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureMpiDaemonEnvsIsMutable();
              mpiDaemonEnvs_.addAll(other.mpiDaemonEnvs_);
            }
            onChanged();
          }
        } else {
          if (!other.mpiDaemonEnvs_.isEmpty()) {
            if (mpiDaemonEnvsBuilder_.isEmpty()) {
              mpiDaemonEnvsBuilder_.dispose();
              mpiDaemonEnvsBuilder_ = null;
              mpiDaemonEnvs_ = other.mpiDaemonEnvs_;
              bitField0_ = (bitField0_ & ~0x00000020);
              mpiDaemonEnvsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getMpiDaemonEnvsFieldBuilder() : null;
            } else {
              mpiDaemonEnvsBuilder_.addAllMessages(other.mpiDaemonEnvs_);
            }
          }
        }
        if (other.hasLocalPeers()) {
          setLocalPeers(other.getLocalPeers());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasResourceId()) {
          
          return false;
        }
        if (!getResourceId().isInitialized()) {
          
          return false;
        }
        for (int i = 0; i < getLocalResourcesCount(); i++) {
          if (!getLocalResources(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .ResourceIdProto resourceId = 1;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder> resourceIdBuilder_;
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public boolean hasResourceId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto getResourceId() {
        if (resourceIdBuilder_ == null) {
          return resourceId_;
        } else {
          return resourceIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public Builder setResourceId(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto value) {
        if (resourceIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          resourceId_ = value;
          onChanged();
        } else {
          resourceIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public Builder setResourceId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder builderForValue) {
        if (resourceIdBuilder_ == null) {
          resourceId_ = builderForValue.build();
          onChanged();
        } else {
          resourceIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public Builder mergeResourceId(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto value) {
        if (resourceIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              resourceId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance()) {
            resourceId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.newBuilder(resourceId_).mergeFrom(value).buildPartial();
          } else {
            resourceId_ = value;
          }
          onChanged();
        } else {
          resourceIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public Builder clearResourceId() {
        if (resourceIdBuilder_ == null) {
          resourceId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.getDefaultInstance();
          onChanged();
        } else {
          resourceIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder getResourceIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getResourceIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder getResourceIdOrBuilder() {
        if (resourceIdBuilder_ != null) {
          return resourceIdBuilder_.getMessageOrBuilder();
        } else {
          return resourceId_;
        }
      }
      /**
       * <code>required .ResourceIdProto resourceId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder> 
          getResourceIdFieldBuilder() {
        if (resourceIdBuilder_ == null) {
          resourceIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceIdProtoOrBuilder>(
                  resourceId_,
                  getParentForChildren(),
                  isClean());
          resourceId_ = null;
        }
        return resourceIdBuilder_;
      }

      // optional string commandline = 2;
      private java.lang.Object commandline_ = "";
      /**
       * <code>optional string commandline = 2;</code>
       */
      public boolean hasCommandline() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string commandline = 2;</code>
       */
      public java.lang.String getCommandline() {
        java.lang.Object ref = commandline_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          commandline_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string commandline = 2;</code>
       */
      public com.google.protobuf.ByteString
          getCommandlineBytes() {
        java.lang.Object ref = commandline_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          commandline_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string commandline = 2;</code>
       */
      public Builder setCommandline(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        commandline_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string commandline = 2;</code>
       */
      public Builder clearCommandline() {
        bitField0_ = (bitField0_ & ~0x00000002);
        commandline_ = getDefaultInstance().getCommandline();
        onChanged();
        return this;
      }
      /**
       * <code>optional string commandline = 2;</code>
       */
      public Builder setCommandlineBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        commandline_ = value;
        onChanged();
        return this;
      }

      // repeated .StringStringMapProto envs = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> envs_ =
        java.util.Collections.emptyList();
      private void ensureEnvsIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          envs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto>(envs_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> envsBuilder_;

      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> getEnvsList() {
        if (envsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(envs_);
        } else {
          return envsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public int getEnvsCount() {
        if (envsBuilder_ == null) {
          return envs_.size();
        } else {
          return envsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto getEnvs(int index) {
        if (envsBuilder_ == null) {
          return envs_.get(index);
        } else {
          return envsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder setEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto value) {
        if (envsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEnvsIsMutable();
          envs_.set(index, value);
          onChanged();
        } else {
          envsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder setEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder builderForValue) {
        if (envsBuilder_ == null) {
          ensureEnvsIsMutable();
          envs_.set(index, builderForValue.build());
          onChanged();
        } else {
          envsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder addEnvs(org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto value) {
        if (envsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEnvsIsMutable();
          envs_.add(value);
          onChanged();
        } else {
          envsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder addEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto value) {
        if (envsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEnvsIsMutable();
          envs_.add(index, value);
          onChanged();
        } else {
          envsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder addEnvs(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder builderForValue) {
        if (envsBuilder_ == null) {
          ensureEnvsIsMutable();
          envs_.add(builderForValue.build());
          onChanged();
        } else {
          envsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder addEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder builderForValue) {
        if (envsBuilder_ == null) {
          ensureEnvsIsMutable();
          envs_.add(index, builderForValue.build());
          onChanged();
        } else {
          envsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder addAllEnvs(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> values) {
        if (envsBuilder_ == null) {
          ensureEnvsIsMutable();
          super.addAll(values, envs_);
          onChanged();
        } else {
          envsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder clearEnvs() {
        if (envsBuilder_ == null) {
          envs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          envsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public Builder removeEnvs(int index) {
        if (envsBuilder_ == null) {
          ensureEnvsIsMutable();
          envs_.remove(index);
          onChanged();
        } else {
          envsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder getEnvsBuilder(
          int index) {
        return getEnvsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder getEnvsOrBuilder(
          int index) {
        if (envsBuilder_ == null) {
          return envs_.get(index);  } else {
          return envsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
           getEnvsOrBuilderList() {
        if (envsBuilder_ != null) {
          return envsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(envs_);
        }
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder addEnvsBuilder() {
        return getEnvsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder addEnvsBuilder(
          int index) {
        return getEnvsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .StringStringMapProto envs = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder> 
           getEnvsBuilderList() {
        return getEnvsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
          getEnvsFieldBuilder() {
        if (envsBuilder_ == null) {
          envsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder>(
                  envs_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          envs_ = null;
        }
        return envsBuilder_;
      }

      // repeated .LocalResourceProto localResources = 4;
      private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto> localResources_ =
        java.util.Collections.emptyList();
      private void ensureLocalResourcesIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          localResources_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto>(localResources_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder> localResourcesBuilder_;

      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto> getLocalResourcesList() {
        if (localResourcesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(localResources_);
        } else {
          return localResourcesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public int getLocalResourcesCount() {
        if (localResourcesBuilder_ == null) {
          return localResources_.size();
        } else {
          return localResourcesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto getLocalResources(int index) {
        if (localResourcesBuilder_ == null) {
          return localResources_.get(index);
        } else {
          return localResourcesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder setLocalResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto value) {
        if (localResourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLocalResourcesIsMutable();
          localResources_.set(index, value);
          onChanged();
        } else {
          localResourcesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder setLocalResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder builderForValue) {
        if (localResourcesBuilder_ == null) {
          ensureLocalResourcesIsMutable();
          localResources_.set(index, builderForValue.build());
          onChanged();
        } else {
          localResourcesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder addLocalResources(org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto value) {
        if (localResourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLocalResourcesIsMutable();
          localResources_.add(value);
          onChanged();
        } else {
          localResourcesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder addLocalResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto value) {
        if (localResourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLocalResourcesIsMutable();
          localResources_.add(index, value);
          onChanged();
        } else {
          localResourcesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder addLocalResources(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder builderForValue) {
        if (localResourcesBuilder_ == null) {
          ensureLocalResourcesIsMutable();
          localResources_.add(builderForValue.build());
          onChanged();
        } else {
          localResourcesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder addLocalResources(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder builderForValue) {
        if (localResourcesBuilder_ == null) {
          ensureLocalResourcesIsMutable();
          localResources_.add(index, builderForValue.build());
          onChanged();
        } else {
          localResourcesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder addAllLocalResources(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto> values) {
        if (localResourcesBuilder_ == null) {
          ensureLocalResourcesIsMutable();
          super.addAll(values, localResources_);
          onChanged();
        } else {
          localResourcesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder clearLocalResources() {
        if (localResourcesBuilder_ == null) {
          localResources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          localResourcesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public Builder removeLocalResources(int index) {
        if (localResourcesBuilder_ == null) {
          ensureLocalResourcesIsMutable();
          localResources_.remove(index);
          onChanged();
        } else {
          localResourcesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder getLocalResourcesBuilder(
          int index) {
        return getLocalResourcesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder getLocalResourcesOrBuilder(
          int index) {
        if (localResourcesBuilder_ == null) {
          return localResources_.get(index);  } else {
          return localResourcesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder> 
           getLocalResourcesOrBuilderList() {
        if (localResourcesBuilder_ != null) {
          return localResourcesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(localResources_);
        }
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder addLocalResourcesBuilder() {
        return getLocalResourcesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.getDefaultInstance());
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder addLocalResourcesBuilder(
          int index) {
        return getLocalResourcesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.getDefaultInstance());
      }
      /**
       * <code>repeated .LocalResourceProto localResources = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder> 
           getLocalResourcesBuilderList() {
        return getLocalResourcesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder> 
          getLocalResourcesFieldBuilder() {
        if (localResourcesBuilder_ == null) {
          localResourcesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.LocalResourceProtoOrBuilder>(
                  localResources_,
                  ((bitField0_ & 0x00000008) == 0x00000008),
                  getParentForChildren(),
                  isClean());
          localResources_ = null;
        }
        return localResourcesBuilder_;
      }

      // optional string mpiDaemonCommandline = 5;
      private java.lang.Object mpiDaemonCommandline_ = "";
      /**
       * <code>optional string mpiDaemonCommandline = 5;</code>
       */
      public boolean hasMpiDaemonCommandline() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional string mpiDaemonCommandline = 5;</code>
       */
      public java.lang.String getMpiDaemonCommandline() {
        java.lang.Object ref = mpiDaemonCommandline_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          mpiDaemonCommandline_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string mpiDaemonCommandline = 5;</code>
       */
      public com.google.protobuf.ByteString
          getMpiDaemonCommandlineBytes() {
        java.lang.Object ref = mpiDaemonCommandline_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mpiDaemonCommandline_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string mpiDaemonCommandline = 5;</code>
       */
      public Builder setMpiDaemonCommandline(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        mpiDaemonCommandline_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string mpiDaemonCommandline = 5;</code>
       */
      public Builder clearMpiDaemonCommandline() {
        bitField0_ = (bitField0_ & ~0x00000010);
        mpiDaemonCommandline_ = getDefaultInstance().getMpiDaemonCommandline();
        onChanged();
        return this;
      }
      /**
       * <code>optional string mpiDaemonCommandline = 5;</code>
       */
      public Builder setMpiDaemonCommandlineBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        mpiDaemonCommandline_ = value;
        onChanged();
        return this;
      }

      // repeated .StringStringMapProto mpiDaemonEnvs = 6;
      private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> mpiDaemonEnvs_ =
        java.util.Collections.emptyList();
      private void ensureMpiDaemonEnvsIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          mpiDaemonEnvs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto>(mpiDaemonEnvs_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> mpiDaemonEnvsBuilder_;

      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> getMpiDaemonEnvsList() {
        if (mpiDaemonEnvsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(mpiDaemonEnvs_);
        } else {
          return mpiDaemonEnvsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public int getMpiDaemonEnvsCount() {
        if (mpiDaemonEnvsBuilder_ == null) {
          return mpiDaemonEnvs_.size();
        } else {
          return mpiDaemonEnvsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto getMpiDaemonEnvs(int index) {
        if (mpiDaemonEnvsBuilder_ == null) {
          return mpiDaemonEnvs_.get(index);
        } else {
          return mpiDaemonEnvsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder setMpiDaemonEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto value) {
        if (mpiDaemonEnvsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMpiDaemonEnvsIsMutable();
          mpiDaemonEnvs_.set(index, value);
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder setMpiDaemonEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder builderForValue) {
        if (mpiDaemonEnvsBuilder_ == null) {
          ensureMpiDaemonEnvsIsMutable();
          mpiDaemonEnvs_.set(index, builderForValue.build());
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder addMpiDaemonEnvs(org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto value) {
        if (mpiDaemonEnvsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMpiDaemonEnvsIsMutable();
          mpiDaemonEnvs_.add(value);
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder addMpiDaemonEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto value) {
        if (mpiDaemonEnvsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMpiDaemonEnvsIsMutable();
          mpiDaemonEnvs_.add(index, value);
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder addMpiDaemonEnvs(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder builderForValue) {
        if (mpiDaemonEnvsBuilder_ == null) {
          ensureMpiDaemonEnvsIsMutable();
          mpiDaemonEnvs_.add(builderForValue.build());
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder addMpiDaemonEnvs(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder builderForValue) {
        if (mpiDaemonEnvsBuilder_ == null) {
          ensureMpiDaemonEnvsIsMutable();
          mpiDaemonEnvs_.add(index, builderForValue.build());
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder addAllMpiDaemonEnvs(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto> values) {
        if (mpiDaemonEnvsBuilder_ == null) {
          ensureMpiDaemonEnvsIsMutable();
          super.addAll(values, mpiDaemonEnvs_);
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder clearMpiDaemonEnvs() {
        if (mpiDaemonEnvsBuilder_ == null) {
          mpiDaemonEnvs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public Builder removeMpiDaemonEnvs(int index) {
        if (mpiDaemonEnvsBuilder_ == null) {
          ensureMpiDaemonEnvsIsMutable();
          mpiDaemonEnvs_.remove(index);
          onChanged();
        } else {
          mpiDaemonEnvsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder getMpiDaemonEnvsBuilder(
          int index) {
        return getMpiDaemonEnvsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder getMpiDaemonEnvsOrBuilder(
          int index) {
        if (mpiDaemonEnvsBuilder_ == null) {
          return mpiDaemonEnvs_.get(index);  } else {
          return mpiDaemonEnvsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
           getMpiDaemonEnvsOrBuilderList() {
        if (mpiDaemonEnvsBuilder_ != null) {
          return mpiDaemonEnvsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(mpiDaemonEnvs_);
        }
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder addMpiDaemonEnvsBuilder() {
        return getMpiDaemonEnvsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder addMpiDaemonEnvsBuilder(
          int index) {
        return getMpiDaemonEnvsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .StringStringMapProto mpiDaemonEnvs = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder> 
           getMpiDaemonEnvsBuilderList() {
        return getMpiDaemonEnvsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder> 
          getMpiDaemonEnvsFieldBuilder() {
        if (mpiDaemonEnvsBuilder_ == null) {
          mpiDaemonEnvsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.StringStringMapProtoOrBuilder>(
                  mpiDaemonEnvs_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          mpiDaemonEnvs_ = null;
        }
        return mpiDaemonEnvsBuilder_;
      }

      // optional int32 localPeers = 7;
      private int localPeers_ ;
      /**
       * <code>optional int32 localPeers = 7;</code>
       */
      public boolean hasLocalPeers() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional int32 localPeers = 7;</code>
       */
      public int getLocalPeers() {
        return localPeers_;
      }
      /**
       * <code>optional int32 localPeers = 7;</code>
       */
      public Builder setLocalPeers(int value) {
        bitField0_ |= 0x00000040;
        localPeers_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 localPeers = 7;</code>
       */
      public Builder clearLocalPeers() {
        bitField0_ = (bitField0_ & ~0x00000040);
        localPeers_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:ResourceLaunchContextProto)
    }

    static {
      defaultInstance = new ResourceLaunchContextProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:ResourceLaunchContextProto)
  }

  public interface CreateJobRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string user = 1;
    /**
     * <code>required string user = 1;</code>
     */
    boolean hasUser();
    /**
     * <code>required string user = 1;</code>
     */
    java.lang.String getUser();
    /**
     * <code>required string user = 1;</code>
     */
    com.google.protobuf.ByteString
        getUserBytes();
  }
  /**
   * Protobuf type {@code CreateJobRequestProto}
   */
  public static final class CreateJobRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements CreateJobRequestProtoOrBuilder {
    // Use CreateJobRequestProto.newBuilder() to construct.
    private CreateJobRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private CreateJobRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final CreateJobRequestProto defaultInstance;
    public static CreateJobRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public CreateJobRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private CreateJobRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              user_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<CreateJobRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<CreateJobRequestProto>() {
      public CreateJobRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CreateJobRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<CreateJobRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string user = 1;
    public static final int USER_FIELD_NUMBER = 1;
    private java.lang.Object user_;
    /**
     * <code>required string user = 1;</code>
     */
    public boolean hasUser() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string user = 1;</code>
     */
    public java.lang.String getUser() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          user_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string user = 1;</code>
     */
    public com.google.protobuf.ByteString
        getUserBytes() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        user_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      user_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasUser()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getUserBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getUserBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto) obj;

      boolean result = true;
      result = result && (hasUser() == other.hasUser());
      if (hasUser()) {
        result = result && getUser()
            .equals(other.getUser());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasUser()) {
        hash = (37 * hash) + USER_FIELD_NUMBER;
        hash = (53 * hash) + getUser().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code CreateJobRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        user_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.user_ = user_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.getDefaultInstance()) return this;
        if (other.hasUser()) {
          bitField0_ |= 0x00000001;
          user_ = other.user_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasUser()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string user = 1;
      private java.lang.Object user_ = "";
      /**
       * <code>required string user = 1;</code>
       */
      public boolean hasUser() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string user = 1;</code>
       */
      public java.lang.String getUser() {
        java.lang.Object ref = user_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          user_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string user = 1;</code>
       */
      public com.google.protobuf.ByteString
          getUserBytes() {
        java.lang.Object ref = user_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          user_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string user = 1;</code>
       */
      public Builder setUser(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        user_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string user = 1;</code>
       */
      public Builder clearUser() {
        bitField0_ = (bitField0_ & ~0x00000001);
        user_ = getDefaultInstance().getUser();
        onChanged();
        return this;
      }
      /**
       * <code>required string user = 1;</code>
       */
      public Builder setUserBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        user_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:CreateJobRequestProto)
    }

    static {
      defaultInstance = new CreateJobRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:CreateJobRequestProto)
  }

  public interface CreateJobResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .JobIdProto jobId = 1;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    boolean hasJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder();
  }
  /**
   * Protobuf type {@code CreateJobResponseProto}
   */
  public static final class CreateJobResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements CreateJobResponseProtoOrBuilder {
    // Use CreateJobResponseProto.newBuilder() to construct.
    private CreateJobResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private CreateJobResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final CreateJobResponseProto defaultInstance;
    public static CreateJobResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public CreateJobResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private CreateJobResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<CreateJobResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<CreateJobResponseProto>() {
      public CreateJobResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CreateJobResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<CreateJobResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .JobIdProto jobId = 1;
    public static final int JOBID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
      return jobId_;
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_;
    }

    private void initFields() {
      jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasJobId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getJobId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, jobId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, jobId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto) obj;

      boolean result = true;
      result = result && (hasJobId() == other.hasJobId());
      if (hasJobId()) {
        result = result && getJobId()
            .equals(other.getJobId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasJobId()) {
        hash = (37 * hash) + JOBID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code CreateJobResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_CreateJobResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (jobIdBuilder_ == null) {
          result.jobId_ = jobId_;
        } else {
          result.jobId_ = jobIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.getDefaultInstance()) return this;
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasJobId()) {
          
          return false;
        }
        if (!getJobId().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .JobIdProto jobId = 1;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              jobId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_;
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder>(
                  jobId_,
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:CreateJobResponseProto)
    }

    static {
      defaultInstance = new CreateJobResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:CreateJobResponseProto)
  }

  public interface HeartbeatRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .JobIdProto jobId = 1;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    boolean hasJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder();
  }
  /**
   * Protobuf type {@code HeartbeatRequestProto}
   */
  public static final class HeartbeatRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements HeartbeatRequestProtoOrBuilder {
    // Use HeartbeatRequestProto.newBuilder() to construct.
    private HeartbeatRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private HeartbeatRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final HeartbeatRequestProto defaultInstance;
    public static HeartbeatRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public HeartbeatRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private HeartbeatRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<HeartbeatRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<HeartbeatRequestProto>() {
      public HeartbeatRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new HeartbeatRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<HeartbeatRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .JobIdProto jobId = 1;
    public static final int JOBID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
      return jobId_;
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_;
    }

    private void initFields() {
      jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasJobId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getJobId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, jobId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, jobId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto) obj;

      boolean result = true;
      result = result && (hasJobId() == other.hasJobId());
      if (hasJobId()) {
        result = result && getJobId()
            .equals(other.getJobId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasJobId()) {
        hash = (37 * hash) + JOBID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code HeartbeatRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (jobIdBuilder_ == null) {
          result.jobId_ = jobId_;
        } else {
          result.jobId_ = jobIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.getDefaultInstance()) return this;
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasJobId()) {
          
          return false;
        }
        if (!getJobId().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .JobIdProto jobId = 1;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              jobId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_;
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder>(
                  jobId_,
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:HeartbeatRequestProto)
    }

    static {
      defaultInstance = new HeartbeatRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:HeartbeatRequestProto)
  }

  public interface HeartbeatResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .MpiJobAllocationUpdateProto updates = 1;
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto> 
        getUpdatesList();
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto getUpdates(int index);
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    int getUpdatesCount();
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder> 
        getUpdatesOrBuilderList();
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder getUpdatesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code HeartbeatResponseProto}
   */
  public static final class HeartbeatResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements HeartbeatResponseProtoOrBuilder {
    // Use HeartbeatResponseProto.newBuilder() to construct.
    private HeartbeatResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private HeartbeatResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final HeartbeatResponseProto defaultInstance;
    public static HeartbeatResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public HeartbeatResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private HeartbeatResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                updates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              updates_.add(input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          updates_ = java.util.Collections.unmodifiableList(updates_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<HeartbeatResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<HeartbeatResponseProto>() {
      public HeartbeatResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new HeartbeatResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<HeartbeatResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .MpiJobAllocationUpdateProto updates = 1;
    public static final int UPDATES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto> updates_;
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto> getUpdatesList() {
      return updates_;
    }
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder> 
        getUpdatesOrBuilderList() {
      return updates_;
    }
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    public int getUpdatesCount() {
      return updates_.size();
    }
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto getUpdates(int index) {
      return updates_.get(index);
    }
    /**
     * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder getUpdatesOrBuilder(
        int index) {
      return updates_.get(index);
    }

    private void initFields() {
      updates_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getUpdatesCount(); i++) {
        if (!getUpdates(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < updates_.size(); i++) {
        output.writeMessage(1, updates_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < updates_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, updates_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto) obj;

      boolean result = true;
      result = result && getUpdatesList()
          .equals(other.getUpdatesList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getUpdatesCount() > 0) {
        hash = (37 * hash) + UPDATES_FIELD_NUMBER;
        hash = (53 * hash) + getUpdatesList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code HeartbeatResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getUpdatesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (updatesBuilder_ == null) {
          updates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          updatesBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_HeartbeatResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (updatesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            updates_ = java.util.Collections.unmodifiableList(updates_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.updates_ = updates_;
        } else {
          result.updates_ = updatesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.getDefaultInstance()) return this;
        if (updatesBuilder_ == null) {
          if (!other.updates_.isEmpty()) {
            if (updates_.isEmpty()) {
              updates_ = other.updates_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureUpdatesIsMutable();
              updates_.addAll(other.updates_);
            }
            onChanged();
          }
        } else {
          if (!other.updates_.isEmpty()) {
            if (updatesBuilder_.isEmpty()) {
              updatesBuilder_.dispose();
              updatesBuilder_ = null;
              updates_ = other.updates_;
              bitField0_ = (bitField0_ & ~0x00000001);
              updatesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getUpdatesFieldBuilder() : null;
            } else {
              updatesBuilder_.addAllMessages(other.updates_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getUpdatesCount(); i++) {
          if (!getUpdates(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .MpiJobAllocationUpdateProto updates = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto> updates_ =
        java.util.Collections.emptyList();
      private void ensureUpdatesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          updates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto>(updates_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder> updatesBuilder_;

      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto> getUpdatesList() {
        if (updatesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(updates_);
        } else {
          return updatesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public int getUpdatesCount() {
        if (updatesBuilder_ == null) {
          return updates_.size();
        } else {
          return updatesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto getUpdates(int index) {
        if (updatesBuilder_ == null) {
          return updates_.get(index);
        } else {
          return updatesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder setUpdates(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto value) {
        if (updatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatesIsMutable();
          updates_.set(index, value);
          onChanged();
        } else {
          updatesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder setUpdates(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder builderForValue) {
        if (updatesBuilder_ == null) {
          ensureUpdatesIsMutable();
          updates_.set(index, builderForValue.build());
          onChanged();
        } else {
          updatesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder addUpdates(org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto value) {
        if (updatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatesIsMutable();
          updates_.add(value);
          onChanged();
        } else {
          updatesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder addUpdates(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto value) {
        if (updatesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatesIsMutable();
          updates_.add(index, value);
          onChanged();
        } else {
          updatesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder addUpdates(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder builderForValue) {
        if (updatesBuilder_ == null) {
          ensureUpdatesIsMutable();
          updates_.add(builderForValue.build());
          onChanged();
        } else {
          updatesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder addUpdates(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder builderForValue) {
        if (updatesBuilder_ == null) {
          ensureUpdatesIsMutable();
          updates_.add(index, builderForValue.build());
          onChanged();
        } else {
          updatesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder addAllUpdates(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto> values) {
        if (updatesBuilder_ == null) {
          ensureUpdatesIsMutable();
          super.addAll(values, updates_);
          onChanged();
        } else {
          updatesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder clearUpdates() {
        if (updatesBuilder_ == null) {
          updates_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          updatesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public Builder removeUpdates(int index) {
        if (updatesBuilder_ == null) {
          ensureUpdatesIsMutable();
          updates_.remove(index);
          onChanged();
        } else {
          updatesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder getUpdatesBuilder(
          int index) {
        return getUpdatesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder getUpdatesOrBuilder(
          int index) {
        if (updatesBuilder_ == null) {
          return updates_.get(index);  } else {
          return updatesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder> 
           getUpdatesOrBuilderList() {
        if (updatesBuilder_ != null) {
          return updatesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(updates_);
        }
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder addUpdatesBuilder() {
        return getUpdatesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.getDefaultInstance());
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder addUpdatesBuilder(
          int index) {
        return getUpdatesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.getDefaultInstance());
      }
      /**
       * <code>repeated .MpiJobAllocationUpdateProto updates = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder> 
           getUpdatesBuilderList() {
        return getUpdatesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder> 
          getUpdatesFieldBuilder() {
        if (updatesBuilder_ == null) {
          updatesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.MpiJobAllocationUpdateProtoOrBuilder>(
                  updates_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          updates_ = null;
        }
        return updatesBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:HeartbeatResponseProto)
    }

    static {
      defaultInstance = new HeartbeatResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:HeartbeatResponseProto)
  }

  public interface FinishJobRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .JobIdProto jobId = 1;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    boolean hasJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder();

    // required .FinalJobStatusProto finalJobStatus = 2;
    /**
     * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
     */
    boolean hasFinalJobStatus();
    /**
     * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto getFinalJobStatus();

    // optional string diagnosticMessage = 3;
    /**
     * <code>optional string diagnosticMessage = 3;</code>
     */
    boolean hasDiagnosticMessage();
    /**
     * <code>optional string diagnosticMessage = 3;</code>
     */
    java.lang.String getDiagnosticMessage();
    /**
     * <code>optional string diagnosticMessage = 3;</code>
     */
    com.google.protobuf.ByteString
        getDiagnosticMessageBytes();
  }
  /**
   * Protobuf type {@code FinishJobRequestProto}
   */
  public static final class FinishJobRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements FinishJobRequestProtoOrBuilder {
    // Use FinishJobRequestProto.newBuilder() to construct.
    private FinishJobRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FinishJobRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FinishJobRequestProto defaultInstance;
    public static FinishJobRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public FinishJobRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FinishJobRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto value = org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                finalJobStatus_ = value;
              }
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              diagnosticMessage_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FinishJobRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<FinishJobRequestProto>() {
      public FinishJobRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FinishJobRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FinishJobRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .JobIdProto jobId = 1;
    public static final int JOBID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
      return jobId_;
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_;
    }

    // required .FinalJobStatusProto finalJobStatus = 2;
    public static final int FINALJOBSTATUS_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto finalJobStatus_;
    /**
     * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
     */
    public boolean hasFinalJobStatus() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto getFinalJobStatus() {
      return finalJobStatus_;
    }

    // optional string diagnosticMessage = 3;
    public static final int DIAGNOSTICMESSAGE_FIELD_NUMBER = 3;
    private java.lang.Object diagnosticMessage_;
    /**
     * <code>optional string diagnosticMessage = 3;</code>
     */
    public boolean hasDiagnosticMessage() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string diagnosticMessage = 3;</code>
     */
    public java.lang.String getDiagnosticMessage() {
      java.lang.Object ref = diagnosticMessage_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnosticMessage_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnosticMessage = 3;</code>
     */
    public com.google.protobuf.ByteString
        getDiagnosticMessageBytes() {
      java.lang.Object ref = diagnosticMessage_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnosticMessage_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      finalJobStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto.JOB_SUCCEEDED;
      diagnosticMessage_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasJobId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasFinalJobStatus()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getJobId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, jobId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, finalJobStatus_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getDiagnosticMessageBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, jobId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, finalJobStatus_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getDiagnosticMessageBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto) obj;

      boolean result = true;
      result = result && (hasJobId() == other.hasJobId());
      if (hasJobId()) {
        result = result && getJobId()
            .equals(other.getJobId());
      }
      result = result && (hasFinalJobStatus() == other.hasFinalJobStatus());
      if (hasFinalJobStatus()) {
        result = result &&
            (getFinalJobStatus() == other.getFinalJobStatus());
      }
      result = result && (hasDiagnosticMessage() == other.hasDiagnosticMessage());
      if (hasDiagnosticMessage()) {
        result = result && getDiagnosticMessage()
            .equals(other.getDiagnosticMessage());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasJobId()) {
        hash = (37 * hash) + JOBID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      if (hasFinalJobStatus()) {
        hash = (37 * hash) + FINALJOBSTATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getFinalJobStatus());
      }
      if (hasDiagnosticMessage()) {
        hash = (37 * hash) + DIAGNOSTICMESSAGE_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnosticMessage().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code FinishJobRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        finalJobStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto.JOB_SUCCEEDED;
        bitField0_ = (bitField0_ & ~0x00000002);
        diagnosticMessage_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (jobIdBuilder_ == null) {
          result.jobId_ = jobId_;
        } else {
          result.jobId_ = jobIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.finalJobStatus_ = finalJobStatus_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.diagnosticMessage_ = diagnosticMessage_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.getDefaultInstance()) return this;
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        if (other.hasFinalJobStatus()) {
          setFinalJobStatus(other.getFinalJobStatus());
        }
        if (other.hasDiagnosticMessage()) {
          bitField0_ |= 0x00000004;
          diagnosticMessage_ = other.diagnosticMessage_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasJobId()) {
          
          return false;
        }
        if (!hasFinalJobStatus()) {
          
          return false;
        }
        if (!getJobId().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .JobIdProto jobId = 1;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              jobId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_;
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder>(
                  jobId_,
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      // required .FinalJobStatusProto finalJobStatus = 2;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto finalJobStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto.JOB_SUCCEEDED;
      /**
       * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
       */
      public boolean hasFinalJobStatus() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto getFinalJobStatus() {
        return finalJobStatus_;
      }
      /**
       * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
       */
      public Builder setFinalJobStatus(org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        finalJobStatus_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .FinalJobStatusProto finalJobStatus = 2;</code>
       */
      public Builder clearFinalJobStatus() {
        bitField0_ = (bitField0_ & ~0x00000002);
        finalJobStatus_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.FinalJobStatusProto.JOB_SUCCEEDED;
        onChanged();
        return this;
      }

      // optional string diagnosticMessage = 3;
      private java.lang.Object diagnosticMessage_ = "";
      /**
       * <code>optional string diagnosticMessage = 3;</code>
       */
      public boolean hasDiagnosticMessage() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string diagnosticMessage = 3;</code>
       */
      public java.lang.String getDiagnosticMessage() {
        java.lang.Object ref = diagnosticMessage_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          diagnosticMessage_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnosticMessage = 3;</code>
       */
      public com.google.protobuf.ByteString
          getDiagnosticMessageBytes() {
        java.lang.Object ref = diagnosticMessage_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnosticMessage_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnosticMessage = 3;</code>
       */
      public Builder setDiagnosticMessage(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        diagnosticMessage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnosticMessage = 3;</code>
       */
      public Builder clearDiagnosticMessage() {
        bitField0_ = (bitField0_ & ~0x00000004);
        diagnosticMessage_ = getDefaultInstance().getDiagnosticMessage();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnosticMessage = 3;</code>
       */
      public Builder setDiagnosticMessageBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        diagnosticMessage_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:FinishJobRequestProto)
    }

    static {
      defaultInstance = new FinishJobRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:FinishJobRequestProto)
  }

  public interface FinishJobResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code FinishJobResponseProto}
   */
  public static final class FinishJobResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements FinishJobResponseProtoOrBuilder {
    // Use FinishJobResponseProto.newBuilder() to construct.
    private FinishJobResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FinishJobResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FinishJobResponseProto defaultInstance;
    public static FinishJobResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public FinishJobResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FinishJobResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FinishJobResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<FinishJobResponseProto>() {
      public FinishJobResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FinishJobResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FinishJobResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code FinishJobResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_FinishJobResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:FinishJobResponseProto)
    }

    static {
      defaultInstance = new FinishJobResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:FinishJobResponseProto)
  }

  public interface AllocateRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string mpiJobName = 1;
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    boolean hasMpiJobName();
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    java.lang.String getMpiJobName();
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    com.google.protobuf.ByteString
        getMpiJobNameBytes();

    // optional int32 memoryInMB = 2;
    /**
     * <code>optional int32 memoryInMB = 2;</code>
     */
    boolean hasMemoryInMB();
    /**
     * <code>optional int32 memoryInMB = 2;</code>
     */
    int getMemoryInMB();

    // optional int32 vCores = 3;
    /**
     * <code>optional int32 vCores = 3;</code>
     */
    boolean hasVCores();
    /**
     * <code>optional int32 vCores = 3;</code>
     */
    int getVCores();

    // optional string policy = 4;
    /**
     * <code>optional string policy = 4;</code>
     */
    boolean hasPolicy();
    /**
     * <code>optional string policy = 4;</code>
     */
    java.lang.String getPolicy();
    /**
     * <code>optional string policy = 4;</code>
     */
    com.google.protobuf.ByteString
        getPolicyBytes();

    // required int32 np = 5;
    /**
     * <code>required int32 np = 5;</code>
     */
    boolean hasNp();
    /**
     * <code>required int32 np = 5;</code>
     */
    int getNp();

    // optional int32 maxProPerNode = 6;
    /**
     * <code>optional int32 maxProPerNode = 6;</code>
     */
    boolean hasMaxProPerNode();
    /**
     * <code>optional int32 maxProPerNode = 6;</code>
     */
    int getMaxProPerNode();

    // optional int32 minProcPerNode = 7;
    /**
     * <code>optional int32 minProcPerNode = 7;</code>
     */
    boolean hasMinProcPerNode();
    /**
     * <code>optional int32 minProcPerNode = 7;</code>
     */
    int getMinProcPerNode();

    // optional int64 timeoutInMs = 8;
    /**
     * <code>optional int64 timeoutInMs = 8;</code>
     */
    boolean hasTimeoutInMs();
    /**
     * <code>optional int64 timeoutInMs = 8;</code>
     */
    long getTimeoutInMs();

    // required .JobIdProto jobId = 9;
    /**
     * <code>required .JobIdProto jobId = 9;</code>
     */
    boolean hasJobId();
    /**
     * <code>required .JobIdProto jobId = 9;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId();
    /**
     * <code>required .JobIdProto jobId = 9;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder();
  }
  /**
   * Protobuf type {@code AllocateRequestProto}
   */
  public static final class AllocateRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements AllocateRequestProtoOrBuilder {
    // Use AllocateRequestProto.newBuilder() to construct.
    private AllocateRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AllocateRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AllocateRequestProto defaultInstance;
    public static AllocateRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public AllocateRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AllocateRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              mpiJobName_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              memoryInMB_ = input.readInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              vCores_ = input.readInt32();
              break;
            }
            case 34: {
              bitField0_ |= 0x00000008;
              policy_ = input.readBytes();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              np_ = input.readInt32();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              maxProPerNode_ = input.readInt32();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              minProcPerNode_ = input.readInt32();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000080;
              timeoutInMs_ = input.readInt64();
              break;
            }
            case 74: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) == 0x00000100)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AllocateRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<AllocateRequestProto>() {
      public AllocateRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllocateRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AllocateRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string mpiJobName = 1;
    public static final int MPIJOBNAME_FIELD_NUMBER = 1;
    private java.lang.Object mpiJobName_;
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    public boolean hasMpiJobName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    public java.lang.String getMpiJobName() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          mpiJobName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string mpiJobName = 1;</code>
     */
    public com.google.protobuf.ByteString
        getMpiJobNameBytes() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mpiJobName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional int32 memoryInMB = 2;
    public static final int MEMORYINMB_FIELD_NUMBER = 2;
    private int memoryInMB_;
    /**
     * <code>optional int32 memoryInMB = 2;</code>
     */
    public boolean hasMemoryInMB() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int32 memoryInMB = 2;</code>
     */
    public int getMemoryInMB() {
      return memoryInMB_;
    }

    // optional int32 vCores = 3;
    public static final int VCORES_FIELD_NUMBER = 3;
    private int vCores_;
    /**
     * <code>optional int32 vCores = 3;</code>
     */
    public boolean hasVCores() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional int32 vCores = 3;</code>
     */
    public int getVCores() {
      return vCores_;
    }

    // optional string policy = 4;
    public static final int POLICY_FIELD_NUMBER = 4;
    private java.lang.Object policy_;
    /**
     * <code>optional string policy = 4;</code>
     */
    public boolean hasPolicy() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional string policy = 4;</code>
     */
    public java.lang.String getPolicy() {
      java.lang.Object ref = policy_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          policy_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string policy = 4;</code>
     */
    public com.google.protobuf.ByteString
        getPolicyBytes() {
      java.lang.Object ref = policy_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        policy_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required int32 np = 5;
    public static final int NP_FIELD_NUMBER = 5;
    private int np_;
    /**
     * <code>required int32 np = 5;</code>
     */
    public boolean hasNp() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>required int32 np = 5;</code>
     */
    public int getNp() {
      return np_;
    }

    // optional int32 maxProPerNode = 6;
    public static final int MAXPROPERNODE_FIELD_NUMBER = 6;
    private int maxProPerNode_;
    /**
     * <code>optional int32 maxProPerNode = 6;</code>
     */
    public boolean hasMaxProPerNode() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional int32 maxProPerNode = 6;</code>
     */
    public int getMaxProPerNode() {
      return maxProPerNode_;
    }

    // optional int32 minProcPerNode = 7;
    public static final int MINPROCPERNODE_FIELD_NUMBER = 7;
    private int minProcPerNode_;
    /**
     * <code>optional int32 minProcPerNode = 7;</code>
     */
    public boolean hasMinProcPerNode() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional int32 minProcPerNode = 7;</code>
     */
    public int getMinProcPerNode() {
      return minProcPerNode_;
    }

    // optional int64 timeoutInMs = 8;
    public static final int TIMEOUTINMS_FIELD_NUMBER = 8;
    private long timeoutInMs_;
    /**
     * <code>optional int64 timeoutInMs = 8;</code>
     */
    public boolean hasTimeoutInMs() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional int64 timeoutInMs = 8;</code>
     */
    public long getTimeoutInMs() {
      return timeoutInMs_;
    }

    // required .JobIdProto jobId = 9;
    public static final int JOBID_FIELD_NUMBER = 9;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_;
    /**
     * <code>required .JobIdProto jobId = 9;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>required .JobIdProto jobId = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
      return jobId_;
    }
    /**
     * <code>required .JobIdProto jobId = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_;
    }

    private void initFields() {
      mpiJobName_ = "";
      memoryInMB_ = 0;
      vCores_ = 0;
      policy_ = "";
      np_ = 0;
      maxProPerNode_ = 0;
      minProcPerNode_ = 0;
      timeoutInMs_ = 0L;
      jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasMpiJobName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasNp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasJobId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getJobId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getMpiJobNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, memoryInMB_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt32(3, vCores_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(4, getPolicyBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeInt32(5, np_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeInt32(6, maxProPerNode_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeInt32(7, minProcPerNode_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeInt64(8, timeoutInMs_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeMessage(9, jobId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getMpiJobNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, memoryInMB_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, vCores_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(4, getPolicyBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(5, np_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(6, maxProPerNode_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, minProcPerNode_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, timeoutInMs_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, jobId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto) obj;

      boolean result = true;
      result = result && (hasMpiJobName() == other.hasMpiJobName());
      if (hasMpiJobName()) {
        result = result && getMpiJobName()
            .equals(other.getMpiJobName());
      }
      result = result && (hasMemoryInMB() == other.hasMemoryInMB());
      if (hasMemoryInMB()) {
        result = result && (getMemoryInMB()
            == other.getMemoryInMB());
      }
      result = result && (hasVCores() == other.hasVCores());
      if (hasVCores()) {
        result = result && (getVCores()
            == other.getVCores());
      }
      result = result && (hasPolicy() == other.hasPolicy());
      if (hasPolicy()) {
        result = result && getPolicy()
            .equals(other.getPolicy());
      }
      result = result && (hasNp() == other.hasNp());
      if (hasNp()) {
        result = result && (getNp()
            == other.getNp());
      }
      result = result && (hasMaxProPerNode() == other.hasMaxProPerNode());
      if (hasMaxProPerNode()) {
        result = result && (getMaxProPerNode()
            == other.getMaxProPerNode());
      }
      result = result && (hasMinProcPerNode() == other.hasMinProcPerNode());
      if (hasMinProcPerNode()) {
        result = result && (getMinProcPerNode()
            == other.getMinProcPerNode());
      }
      result = result && (hasTimeoutInMs() == other.hasTimeoutInMs());
      if (hasTimeoutInMs()) {
        result = result && (getTimeoutInMs()
            == other.getTimeoutInMs());
      }
      result = result && (hasJobId() == other.hasJobId());
      if (hasJobId()) {
        result = result && getJobId()
            .equals(other.getJobId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasMpiJobName()) {
        hash = (37 * hash) + MPIJOBNAME_FIELD_NUMBER;
        hash = (53 * hash) + getMpiJobName().hashCode();
      }
      if (hasMemoryInMB()) {
        hash = (37 * hash) + MEMORYINMB_FIELD_NUMBER;
        hash = (53 * hash) + getMemoryInMB();
      }
      if (hasVCores()) {
        hash = (37 * hash) + VCORES_FIELD_NUMBER;
        hash = (53 * hash) + getVCores();
      }
      if (hasPolicy()) {
        hash = (37 * hash) + POLICY_FIELD_NUMBER;
        hash = (53 * hash) + getPolicy().hashCode();
      }
      if (hasNp()) {
        hash = (37 * hash) + NP_FIELD_NUMBER;
        hash = (53 * hash) + getNp();
      }
      if (hasMaxProPerNode()) {
        hash = (37 * hash) + MAXPROPERNODE_FIELD_NUMBER;
        hash = (53 * hash) + getMaxProPerNode();
      }
      if (hasMinProcPerNode()) {
        hash = (37 * hash) + MINPROCPERNODE_FIELD_NUMBER;
        hash = (53 * hash) + getMinProcPerNode();
      }
      if (hasTimeoutInMs()) {
        hash = (37 * hash) + TIMEOUTINMS_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimeoutInMs());
      }
      if (hasJobId()) {
        hash = (37 * hash) + JOBID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code AllocateRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        mpiJobName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        memoryInMB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        vCores_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        policy_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        np_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        maxProPerNode_ = 0;
        bitField0_ = (bitField0_ & ~0x00000020);
        minProcPerNode_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        timeoutInMs_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.mpiJobName_ = mpiJobName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.memoryInMB_ = memoryInMB_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.vCores_ = vCores_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.policy_ = policy_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.np_ = np_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.maxProPerNode_ = maxProPerNode_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.minProcPerNode_ = minProcPerNode_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        result.timeoutInMs_ = timeoutInMs_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000100;
        }
        if (jobIdBuilder_ == null) {
          result.jobId_ = jobId_;
        } else {
          result.jobId_ = jobIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.getDefaultInstance()) return this;
        if (other.hasMpiJobName()) {
          bitField0_ |= 0x00000001;
          mpiJobName_ = other.mpiJobName_;
          onChanged();
        }
        if (other.hasMemoryInMB()) {
          setMemoryInMB(other.getMemoryInMB());
        }
        if (other.hasVCores()) {
          setVCores(other.getVCores());
        }
        if (other.hasPolicy()) {
          bitField0_ |= 0x00000008;
          policy_ = other.policy_;
          onChanged();
        }
        if (other.hasNp()) {
          setNp(other.getNp());
        }
        if (other.hasMaxProPerNode()) {
          setMaxProPerNode(other.getMaxProPerNode());
        }
        if (other.hasMinProcPerNode()) {
          setMinProcPerNode(other.getMinProcPerNode());
        }
        if (other.hasTimeoutInMs()) {
          setTimeoutInMs(other.getTimeoutInMs());
        }
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasMpiJobName()) {
          
          return false;
        }
        if (!hasNp()) {
          
          return false;
        }
        if (!hasJobId()) {
          
          return false;
        }
        if (!getJobId().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string mpiJobName = 1;
      private java.lang.Object mpiJobName_ = "";
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public boolean hasMpiJobName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public java.lang.String getMpiJobName() {
        java.lang.Object ref = mpiJobName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          mpiJobName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public com.google.protobuf.ByteString
          getMpiJobNameBytes() {
        java.lang.Object ref = mpiJobName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mpiJobName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public Builder setMpiJobName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        mpiJobName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public Builder clearMpiJobName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        mpiJobName_ = getDefaultInstance().getMpiJobName();
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 1;</code>
       */
      public Builder setMpiJobNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        mpiJobName_ = value;
        onChanged();
        return this;
      }

      // optional int32 memoryInMB = 2;
      private int memoryInMB_ ;
      /**
       * <code>optional int32 memoryInMB = 2;</code>
       */
      public boolean hasMemoryInMB() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional int32 memoryInMB = 2;</code>
       */
      public int getMemoryInMB() {
        return memoryInMB_;
      }
      /**
       * <code>optional int32 memoryInMB = 2;</code>
       */
      public Builder setMemoryInMB(int value) {
        bitField0_ |= 0x00000002;
        memoryInMB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 memoryInMB = 2;</code>
       */
      public Builder clearMemoryInMB() {
        bitField0_ = (bitField0_ & ~0x00000002);
        memoryInMB_ = 0;
        onChanged();
        return this;
      }

      // optional int32 vCores = 3;
      private int vCores_ ;
      /**
       * <code>optional int32 vCores = 3;</code>
       */
      public boolean hasVCores() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional int32 vCores = 3;</code>
       */
      public int getVCores() {
        return vCores_;
      }
      /**
       * <code>optional int32 vCores = 3;</code>
       */
      public Builder setVCores(int value) {
        bitField0_ |= 0x00000004;
        vCores_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 vCores = 3;</code>
       */
      public Builder clearVCores() {
        bitField0_ = (bitField0_ & ~0x00000004);
        vCores_ = 0;
        onChanged();
        return this;
      }

      // optional string policy = 4;
      private java.lang.Object policy_ = "";
      /**
       * <code>optional string policy = 4;</code>
       */
      public boolean hasPolicy() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional string policy = 4;</code>
       */
      public java.lang.String getPolicy() {
        java.lang.Object ref = policy_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          policy_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string policy = 4;</code>
       */
      public com.google.protobuf.ByteString
          getPolicyBytes() {
        java.lang.Object ref = policy_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          policy_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string policy = 4;</code>
       */
      public Builder setPolicy(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        policy_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string policy = 4;</code>
       */
      public Builder clearPolicy() {
        bitField0_ = (bitField0_ & ~0x00000008);
        policy_ = getDefaultInstance().getPolicy();
        onChanged();
        return this;
      }
      /**
       * <code>optional string policy = 4;</code>
       */
      public Builder setPolicyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        policy_ = value;
        onChanged();
        return this;
      }

      // required int32 np = 5;
      private int np_ ;
      /**
       * <code>required int32 np = 5;</code>
       */
      public boolean hasNp() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>required int32 np = 5;</code>
       */
      public int getNp() {
        return np_;
      }
      /**
       * <code>required int32 np = 5;</code>
       */
      public Builder setNp(int value) {
        bitField0_ |= 0x00000010;
        np_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int32 np = 5;</code>
       */
      public Builder clearNp() {
        bitField0_ = (bitField0_ & ~0x00000010);
        np_ = 0;
        onChanged();
        return this;
      }

      // optional int32 maxProPerNode = 6;
      private int maxProPerNode_ ;
      /**
       * <code>optional int32 maxProPerNode = 6;</code>
       */
      public boolean hasMaxProPerNode() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional int32 maxProPerNode = 6;</code>
       */
      public int getMaxProPerNode() {
        return maxProPerNode_;
      }
      /**
       * <code>optional int32 maxProPerNode = 6;</code>
       */
      public Builder setMaxProPerNode(int value) {
        bitField0_ |= 0x00000020;
        maxProPerNode_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 maxProPerNode = 6;</code>
       */
      public Builder clearMaxProPerNode() {
        bitField0_ = (bitField0_ & ~0x00000020);
        maxProPerNode_ = 0;
        onChanged();
        return this;
      }

      // optional int32 minProcPerNode = 7;
      private int minProcPerNode_ ;
      /**
       * <code>optional int32 minProcPerNode = 7;</code>
       */
      public boolean hasMinProcPerNode() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional int32 minProcPerNode = 7;</code>
       */
      public int getMinProcPerNode() {
        return minProcPerNode_;
      }
      /**
       * <code>optional int32 minProcPerNode = 7;</code>
       */
      public Builder setMinProcPerNode(int value) {
        bitField0_ |= 0x00000040;
        minProcPerNode_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 minProcPerNode = 7;</code>
       */
      public Builder clearMinProcPerNode() {
        bitField0_ = (bitField0_ & ~0x00000040);
        minProcPerNode_ = 0;
        onChanged();
        return this;
      }

      // optional int64 timeoutInMs = 8;
      private long timeoutInMs_ ;
      /**
       * <code>optional int64 timeoutInMs = 8;</code>
       */
      public boolean hasTimeoutInMs() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional int64 timeoutInMs = 8;</code>
       */
      public long getTimeoutInMs() {
        return timeoutInMs_;
      }
      /**
       * <code>optional int64 timeoutInMs = 8;</code>
       */
      public Builder setTimeoutInMs(long value) {
        bitField0_ |= 0x00000080;
        timeoutInMs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 timeoutInMs = 8;</code>
       */
      public Builder clearTimeoutInMs() {
        bitField0_ = (bitField0_ & ~0x00000080);
        timeoutInMs_ = 0L;
        onChanged();
        return this;
      }

      // required .JobIdProto jobId = 9;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public Builder setJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000100) == 0x00000100) &&
              jobId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000100;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_;
        }
      }
      /**
       * <code>required .JobIdProto jobId = 9;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder>(
                  jobId_,
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:AllocateRequestProto)
    }

    static {
      defaultInstance = new AllocateRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:AllocateRequestProto)
  }

  public interface AllocateResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code AllocateResponseProto}
   */
  public static final class AllocateResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements AllocateResponseProtoOrBuilder {
    // Use AllocateResponseProto.newBuilder() to construct.
    private AllocateResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AllocateResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AllocateResponseProto defaultInstance;
    public static AllocateResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public AllocateResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AllocateResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AllocateResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<AllocateResponseProto>() {
      public AllocateResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllocateResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AllocateResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code AllocateResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_AllocateResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:AllocateResponseProto)
    }

    static {
      defaultInstance = new AllocateResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:AllocateResponseProto)
  }

  public interface LaunchRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .JobIdProto jobId = 1;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    boolean hasJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder();

    // required string mpiJobName = 2;
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    boolean hasMpiJobName();
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    java.lang.String getMpiJobName();
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    com.google.protobuf.ByteString
        getMpiJobNameBytes();

    // repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto> 
        getResourceLaunchContextsList();
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto getResourceLaunchContexts(int index);
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    int getResourceLaunchContextsCount();
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder> 
        getResourceLaunchContextsOrBuilderList();
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder getResourceLaunchContextsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code LaunchRequestProto}
   */
  public static final class LaunchRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements LaunchRequestProtoOrBuilder {
    // Use LaunchRequestProto.newBuilder() to construct.
    private LaunchRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private LaunchRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final LaunchRequestProto defaultInstance;
    public static LaunchRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public LaunchRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private LaunchRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              mpiJobName_ = input.readBytes();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                resourceLaunchContexts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              resourceLaunchContexts_.add(input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          resourceLaunchContexts_ = java.util.Collections.unmodifiableList(resourceLaunchContexts_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<LaunchRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<LaunchRequestProto>() {
      public LaunchRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new LaunchRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<LaunchRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .JobIdProto jobId = 1;
    public static final int JOBID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
      return jobId_;
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_;
    }

    // required string mpiJobName = 2;
    public static final int MPIJOBNAME_FIELD_NUMBER = 2;
    private java.lang.Object mpiJobName_;
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    public boolean hasMpiJobName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    public java.lang.String getMpiJobName() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          mpiJobName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    public com.google.protobuf.ByteString
        getMpiJobNameBytes() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mpiJobName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;
    public static final int RESOURCELAUNCHCONTEXTS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto> resourceLaunchContexts_;
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto> getResourceLaunchContextsList() {
      return resourceLaunchContexts_;
    }
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder> 
        getResourceLaunchContextsOrBuilderList() {
      return resourceLaunchContexts_;
    }
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    public int getResourceLaunchContextsCount() {
      return resourceLaunchContexts_.size();
    }
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto getResourceLaunchContexts(int index) {
      return resourceLaunchContexts_.get(index);
    }
    /**
     * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder getResourceLaunchContextsOrBuilder(
        int index) {
      return resourceLaunchContexts_.get(index);
    }

    private void initFields() {
      jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      mpiJobName_ = "";
      resourceLaunchContexts_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasJobId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasMpiJobName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getJobId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getResourceLaunchContextsCount(); i++) {
        if (!getResourceLaunchContexts(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, jobId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getMpiJobNameBytes());
      }
      for (int i = 0; i < resourceLaunchContexts_.size(); i++) {
        output.writeMessage(3, resourceLaunchContexts_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, jobId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getMpiJobNameBytes());
      }
      for (int i = 0; i < resourceLaunchContexts_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, resourceLaunchContexts_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto) obj;

      boolean result = true;
      result = result && (hasJobId() == other.hasJobId());
      if (hasJobId()) {
        result = result && getJobId()
            .equals(other.getJobId());
      }
      result = result && (hasMpiJobName() == other.hasMpiJobName());
      if (hasMpiJobName()) {
        result = result && getMpiJobName()
            .equals(other.getMpiJobName());
      }
      result = result && getResourceLaunchContextsList()
          .equals(other.getResourceLaunchContextsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasJobId()) {
        hash = (37 * hash) + JOBID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      if (hasMpiJobName()) {
        hash = (37 * hash) + MPIJOBNAME_FIELD_NUMBER;
        hash = (53 * hash) + getMpiJobName().hashCode();
      }
      if (getResourceLaunchContextsCount() > 0) {
        hash = (37 * hash) + RESOURCELAUNCHCONTEXTS_FIELD_NUMBER;
        hash = (53 * hash) + getResourceLaunchContextsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code LaunchRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
          getResourceLaunchContextsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        mpiJobName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (resourceLaunchContextsBuilder_ == null) {
          resourceLaunchContexts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          resourceLaunchContextsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (jobIdBuilder_ == null) {
          result.jobId_ = jobId_;
        } else {
          result.jobId_ = jobIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.mpiJobName_ = mpiJobName_;
        if (resourceLaunchContextsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            resourceLaunchContexts_ = java.util.Collections.unmodifiableList(resourceLaunchContexts_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.resourceLaunchContexts_ = resourceLaunchContexts_;
        } else {
          result.resourceLaunchContexts_ = resourceLaunchContextsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.getDefaultInstance()) return this;
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        if (other.hasMpiJobName()) {
          bitField0_ |= 0x00000002;
          mpiJobName_ = other.mpiJobName_;
          onChanged();
        }
        if (resourceLaunchContextsBuilder_ == null) {
          if (!other.resourceLaunchContexts_.isEmpty()) {
            if (resourceLaunchContexts_.isEmpty()) {
              resourceLaunchContexts_ = other.resourceLaunchContexts_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureResourceLaunchContextsIsMutable();
              resourceLaunchContexts_.addAll(other.resourceLaunchContexts_);
            }
            onChanged();
          }
        } else {
          if (!other.resourceLaunchContexts_.isEmpty()) {
            if (resourceLaunchContextsBuilder_.isEmpty()) {
              resourceLaunchContextsBuilder_.dispose();
              resourceLaunchContextsBuilder_ = null;
              resourceLaunchContexts_ = other.resourceLaunchContexts_;
              bitField0_ = (bitField0_ & ~0x00000004);
              resourceLaunchContextsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getResourceLaunchContextsFieldBuilder() : null;
            } else {
              resourceLaunchContextsBuilder_.addAllMessages(other.resourceLaunchContexts_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasJobId()) {
          
          return false;
        }
        if (!hasMpiJobName()) {
          
          return false;
        }
        if (!getJobId().isInitialized()) {
          
          return false;
        }
        for (int i = 0; i < getResourceLaunchContextsCount(); i++) {
          if (!getResourceLaunchContexts(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .JobIdProto jobId = 1;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              jobId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_;
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder>(
                  jobId_,
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      // required string mpiJobName = 2;
      private java.lang.Object mpiJobName_ = "";
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public boolean hasMpiJobName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public java.lang.String getMpiJobName() {
        java.lang.Object ref = mpiJobName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          mpiJobName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public com.google.protobuf.ByteString
          getMpiJobNameBytes() {
        java.lang.Object ref = mpiJobName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mpiJobName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public Builder setMpiJobName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        mpiJobName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public Builder clearMpiJobName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        mpiJobName_ = getDefaultInstance().getMpiJobName();
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public Builder setMpiJobNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        mpiJobName_ = value;
        onChanged();
        return this;
      }

      // repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto> resourceLaunchContexts_ =
        java.util.Collections.emptyList();
      private void ensureResourceLaunchContextsIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          resourceLaunchContexts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto>(resourceLaunchContexts_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder> resourceLaunchContextsBuilder_;

      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto> getResourceLaunchContextsList() {
        if (resourceLaunchContextsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(resourceLaunchContexts_);
        } else {
          return resourceLaunchContextsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public int getResourceLaunchContextsCount() {
        if (resourceLaunchContextsBuilder_ == null) {
          return resourceLaunchContexts_.size();
        } else {
          return resourceLaunchContextsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto getResourceLaunchContexts(int index) {
        if (resourceLaunchContextsBuilder_ == null) {
          return resourceLaunchContexts_.get(index);
        } else {
          return resourceLaunchContextsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder setResourceLaunchContexts(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto value) {
        if (resourceLaunchContextsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourceLaunchContextsIsMutable();
          resourceLaunchContexts_.set(index, value);
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder setResourceLaunchContexts(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder builderForValue) {
        if (resourceLaunchContextsBuilder_ == null) {
          ensureResourceLaunchContextsIsMutable();
          resourceLaunchContexts_.set(index, builderForValue.build());
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder addResourceLaunchContexts(org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto value) {
        if (resourceLaunchContextsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourceLaunchContextsIsMutable();
          resourceLaunchContexts_.add(value);
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder addResourceLaunchContexts(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto value) {
        if (resourceLaunchContextsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourceLaunchContextsIsMutable();
          resourceLaunchContexts_.add(index, value);
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder addResourceLaunchContexts(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder builderForValue) {
        if (resourceLaunchContextsBuilder_ == null) {
          ensureResourceLaunchContextsIsMutable();
          resourceLaunchContexts_.add(builderForValue.build());
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder addResourceLaunchContexts(
          int index, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder builderForValue) {
        if (resourceLaunchContextsBuilder_ == null) {
          ensureResourceLaunchContextsIsMutable();
          resourceLaunchContexts_.add(index, builderForValue.build());
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder addAllResourceLaunchContexts(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto> values) {
        if (resourceLaunchContextsBuilder_ == null) {
          ensureResourceLaunchContextsIsMutable();
          super.addAll(values, resourceLaunchContexts_);
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder clearResourceLaunchContexts() {
        if (resourceLaunchContextsBuilder_ == null) {
          resourceLaunchContexts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public Builder removeResourceLaunchContexts(int index) {
        if (resourceLaunchContextsBuilder_ == null) {
          ensureResourceLaunchContextsIsMutable();
          resourceLaunchContexts_.remove(index);
          onChanged();
        } else {
          resourceLaunchContextsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder getResourceLaunchContextsBuilder(
          int index) {
        return getResourceLaunchContextsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder getResourceLaunchContextsOrBuilder(
          int index) {
        if (resourceLaunchContextsBuilder_ == null) {
          return resourceLaunchContexts_.get(index);  } else {
          return resourceLaunchContextsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder> 
           getResourceLaunchContextsOrBuilderList() {
        if (resourceLaunchContextsBuilder_ != null) {
          return resourceLaunchContextsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(resourceLaunchContexts_);
        }
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder addResourceLaunchContextsBuilder() {
        return getResourceLaunchContextsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.getDefaultInstance());
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder addResourceLaunchContextsBuilder(
          int index) {
        return getResourceLaunchContextsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.getDefaultInstance());
      }
      /**
       * <code>repeated .ResourceLaunchContextProto resourceLaunchContexts = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder> 
           getResourceLaunchContextsBuilderList() {
        return getResourceLaunchContextsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder> 
          getResourceLaunchContextsFieldBuilder() {
        if (resourceLaunchContextsBuilder_ == null) {
          resourceLaunchContextsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.ResourceLaunchContextProtoOrBuilder>(
                  resourceLaunchContexts_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          resourceLaunchContexts_ = null;
        }
        return resourceLaunchContextsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:LaunchRequestProto)
    }

    static {
      defaultInstance = new LaunchRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:LaunchRequestProto)
  }

  public interface LaunchResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code LaunchResponseProto}
   */
  public static final class LaunchResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements LaunchResponseProtoOrBuilder {
    // Use LaunchResponseProto.newBuilder() to construct.
    private LaunchResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private LaunchResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final LaunchResponseProto defaultInstance;
    public static LaunchResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public LaunchResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private LaunchResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<LaunchResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<LaunchResponseProto>() {
      public LaunchResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new LaunchResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<LaunchResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code LaunchResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_LaunchResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:LaunchResponseProto)
    }

    static {
      defaultInstance = new LaunchResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:LaunchResponseProto)
  }

  public interface TerminateRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .JobIdProto jobId = 1;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    boolean hasJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId();
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder();

    // required string mpiJobName = 2;
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    boolean hasMpiJobName();
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    java.lang.String getMpiJobName();
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    com.google.protobuf.ByteString
        getMpiJobNameBytes();
  }
  /**
   * Protobuf type {@code TerminateRequestProto}
   */
  public static final class TerminateRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements TerminateRequestProtoOrBuilder {
    // Use TerminateRequestProto.newBuilder() to construct.
    private TerminateRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private TerminateRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final TerminateRequestProto defaultInstance;
    public static TerminateRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public TerminateRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private TerminateRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              mpiJobName_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<TerminateRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<TerminateRequestProto>() {
      public TerminateRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TerminateRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<TerminateRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .JobIdProto jobId = 1;
    public static final int JOBID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_;
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
      return jobId_;
    }
    /**
     * <code>required .JobIdProto jobId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_;
    }

    // required string mpiJobName = 2;
    public static final int MPIJOBNAME_FIELD_NUMBER = 2;
    private java.lang.Object mpiJobName_;
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    public boolean hasMpiJobName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    public java.lang.String getMpiJobName() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          mpiJobName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string mpiJobName = 2;</code>
     */
    public com.google.protobuf.ByteString
        getMpiJobNameBytes() {
      java.lang.Object ref = mpiJobName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mpiJobName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      mpiJobName_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasJobId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasMpiJobName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getJobId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, jobId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getMpiJobNameBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, jobId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getMpiJobNameBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto) obj;

      boolean result = true;
      result = result && (hasJobId() == other.hasJobId());
      if (hasJobId()) {
        result = result && getJobId()
            .equals(other.getJobId());
      }
      result = result && (hasMpiJobName() == other.hasMpiJobName());
      if (hasMpiJobName()) {
        result = result && getMpiJobName()
            .equals(other.getMpiJobName());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasJobId()) {
        hash = (37 * hash) + JOBID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      if (hasMpiJobName()) {
        hash = (37 * hash) + MPIJOBNAME_FIELD_NUMBER;
        hash = (53 * hash) + getMpiJobName().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code TerminateRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        mpiJobName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (jobIdBuilder_ == null) {
          result.jobId_ = jobId_;
        } else {
          result.jobId_ = jobIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.mpiJobName_ = mpiJobName_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.getDefaultInstance()) return this;
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        if (other.hasMpiJobName()) {
          bitField0_ |= 0x00000002;
          mpiJobName_ = other.mpiJobName_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasJobId()) {
          
          return false;
        }
        if (!hasMpiJobName()) {
          
          return false;
        }
        if (!getJobId().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .JobIdProto jobId = 1;
      private org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              jobId_ != org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.getDefaultInstance();
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_;
        }
      }
      /**
       * <code>required .JobIdProto jobId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProto.Builder, org.apache.hadoop.yarn.proto.JobMasterProtocol.JobIdProtoOrBuilder>(
                  jobId_,
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      // required string mpiJobName = 2;
      private java.lang.Object mpiJobName_ = "";
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public boolean hasMpiJobName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public java.lang.String getMpiJobName() {
        java.lang.Object ref = mpiJobName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          mpiJobName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public com.google.protobuf.ByteString
          getMpiJobNameBytes() {
        java.lang.Object ref = mpiJobName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mpiJobName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public Builder setMpiJobName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        mpiJobName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public Builder clearMpiJobName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        mpiJobName_ = getDefaultInstance().getMpiJobName();
        onChanged();
        return this;
      }
      /**
       * <code>required string mpiJobName = 2;</code>
       */
      public Builder setMpiJobNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        mpiJobName_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:TerminateRequestProto)
    }

    static {
      defaultInstance = new TerminateRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:TerminateRequestProto)
  }

  public interface TerminateResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code TerminateResponseProto}
   */
  public static final class TerminateResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements TerminateResponseProtoOrBuilder {
    // Use TerminateResponseProto.newBuilder() to construct.
    private TerminateResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private TerminateResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final TerminateResponseProto defaultInstance;
    public static TerminateResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public TerminateResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private TerminateResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<TerminateResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<TerminateResponseProto>() {
      public TerminateResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TerminateResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<TerminateResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto other = (org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code TerminateResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.class, org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.internal_static_TerminateResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto build() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto result = new org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:TerminateResponseProto)
    }

    static {
      defaultInstance = new TerminateResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:TerminateResponseProto)
  }

  /**
   * Protobuf service {@code JobMasterProtocolService}
   */
  public static abstract class JobMasterProtocolService
      implements com.google.protobuf.Service {
    protected JobMasterProtocolService() {}

    public interface Interface {
      /**
       * <code>rpc createJob(.CreateJobRequestProto) returns (.CreateJobResponseProto);</code>
       */
      public abstract void createJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto> done);

      /**
       * <code>rpc heartbeat(.HeartbeatRequestProto) returns (.HeartbeatResponseProto);</code>
       */
      public abstract void heartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto> done);

      /**
       * <code>rpc finishJob(.FinishJobRequestProto) returns (.FinishJobResponseProto);</code>
       */
      public abstract void finishJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto> done);

      /**
       * <code>rpc allocate(.AllocateRequestProto) returns (.AllocateResponseProto);</code>
       */
      public abstract void allocate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto> done);

      /**
       * <code>rpc launch(.LaunchRequestProto) returns (.LaunchResponseProto);</code>
       */
      public abstract void launch(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto> done);

      /**
       * <code>rpc terminate(.TerminateRequestProto) returns (.TerminateResponseProto);</code>
       */
      public abstract void terminate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto> done);

    }

    public static com.google.protobuf.Service newReflectiveService(
        final Interface impl) {
      return new JobMasterProtocolService() {
        @java.lang.Override
        public  void createJob(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto> done) {
          impl.createJob(controller, request, done);
        }

        @java.lang.Override
        public  void heartbeat(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto> done) {
          impl.heartbeat(controller, request, done);
        }

        @java.lang.Override
        public  void finishJob(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto> done) {
          impl.finishJob(controller, request, done);
        }

        @java.lang.Override
        public  void allocate(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto> done) {
          impl.allocate(controller, request, done);
        }

        @java.lang.Override
        public  void launch(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto> done) {
          impl.launch(controller, request, done);
        }

        @java.lang.Override
        public  void terminate(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto> done) {
          impl.terminate(controller, request, done);
        }

      };
    }

    public static com.google.protobuf.BlockingService
        newReflectiveBlockingService(final BlockingInterface impl) {
      return new com.google.protobuf.BlockingService() {
        public final com.google.protobuf.Descriptors.ServiceDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }

        public final com.google.protobuf.Message callBlockingMethod(
            com.google.protobuf.Descriptors.MethodDescriptor method,
            com.google.protobuf.RpcController controller,
            com.google.protobuf.Message request)
            throws com.google.protobuf.ServiceException {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.callBlockingMethod() given method descriptor for " +
              "wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return impl.createJob(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto)request);
            case 1:
              return impl.heartbeat(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto)request);
            case 2:
              return impl.finishJob(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto)request);
            case 3:
              return impl.allocate(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto)request);
            case 4:
              return impl.launch(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto)request);
            case 5:
              return impl.terminate(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto)request);
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getRequestPrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getRequestPrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getResponsePrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getResponsePrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

      };
    }

    /**
     * <code>rpc createJob(.CreateJobRequestProto) returns (.CreateJobResponseProto);</code>
     */
    public abstract void createJob(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto> done);

    /**
     * <code>rpc heartbeat(.HeartbeatRequestProto) returns (.HeartbeatResponseProto);</code>
     */
    public abstract void heartbeat(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto> done);

    /**
     * <code>rpc finishJob(.FinishJobRequestProto) returns (.FinishJobResponseProto);</code>
     */
    public abstract void finishJob(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto> done);

    /**
     * <code>rpc allocate(.AllocateRequestProto) returns (.AllocateResponseProto);</code>
     */
    public abstract void allocate(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto> done);

    /**
     * <code>rpc launch(.LaunchRequestProto) returns (.LaunchResponseProto);</code>
     */
    public abstract void launch(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto> done);

    /**
     * <code>rpc terminate(.TerminateRequestProto) returns (.TerminateResponseProto);</code>
     */
    public abstract void terminate(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto> done);

    public static final
        com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.JobMasterProtocol.getDescriptor().getServices().get(0);
    }
    public final com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }

    public final void callMethod(
        com.google.protobuf.Descriptors.MethodDescriptor method,
        com.google.protobuf.RpcController controller,
        com.google.protobuf.Message request,
        com.google.protobuf.RpcCallback<
          com.google.protobuf.Message> done) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.callMethod() given method descriptor for wrong " +
          "service type.");
      }
      switch(method.getIndex()) {
        case 0:
          this.createJob(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto>specializeCallback(
              done));
          return;
        case 1:
          this.heartbeat(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto>specializeCallback(
              done));
          return;
        case 2:
          this.finishJob(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto>specializeCallback(
              done));
          return;
        case 3:
          this.allocate(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto>specializeCallback(
              done));
          return;
        case 4:
          this.launch(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto>specializeCallback(
              done));
          return;
        case 5:
          this.terminate(controller, (org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto>specializeCallback(
              done));
          return;
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getRequestPrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getRequestPrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getResponsePrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getResponsePrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public static Stub newStub(
        com.google.protobuf.RpcChannel channel) {
      return new Stub(channel);
    }

    public static final class Stub extends org.apache.hadoop.yarn.proto.JobMasterProtocol.JobMasterProtocolService implements Interface {
      private Stub(com.google.protobuf.RpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.RpcChannel channel;

      public com.google.protobuf.RpcChannel getChannel() {
        return channel;
      }

      public  void createJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.class,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.getDefaultInstance()));
      }

      public  void heartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.class,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.getDefaultInstance()));
      }

      public  void finishJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.class,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.getDefaultInstance()));
      }

      public  void allocate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.class,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.getDefaultInstance()));
      }

      public  void launch(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.class,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.getDefaultInstance()));
      }

      public  void terminate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.class,
            org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.getDefaultInstance()));
      }
    }

    public static BlockingInterface newBlockingStub(
        com.google.protobuf.BlockingRpcChannel channel) {
      return new BlockingStub(channel);
    }

    public interface BlockingInterface {
      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto createJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto heartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto finishJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto allocate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto launch(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto terminate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto request)
          throws com.google.protobuf.ServiceException;
    }

    private static final class BlockingStub implements BlockingInterface {
      private BlockingStub(com.google.protobuf.BlockingRpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.BlockingRpcChannel channel;

      public org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto createJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.CreateJobResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto heartbeat(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.HeartbeatResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto finishJob(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.FinishJobResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto allocate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.AllocateResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto launch(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.LaunchResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto terminate(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.yarn.proto.JobMasterProtocol.TerminateResponseProto.getDefaultInstance());
      }

    }

    // @@protoc_insertion_point(class_scope:JobMasterProtocolService)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_JobIdProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_JobIdProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_ResourceIdProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_ResourceIdProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_AllocatedResourceProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_AllocatedResourceProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_MpiJobAllocationUpdateProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_MpiJobAllocationUpdateProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_StringStringMapProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_StringStringMapProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LocalResourceProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LocalResourceProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_ResourceLaunchContextProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_ResourceLaunchContextProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_CreateJobRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_CreateJobRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_CreateJobResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_CreateJobResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_HeartbeatRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_HeartbeatRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_HeartbeatResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_HeartbeatResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_FinishJobRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_FinishJobRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_FinishJobResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_FinishJobResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_AllocateRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_AllocateRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_AllocateResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_AllocateResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LaunchRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LaunchRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LaunchResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LaunchResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_TerminateRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_TerminateRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_TerminateResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_TerminateResponseProto_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\024hamster-protos.proto\"+\n\nJobIdProto\022\021\n\t" +
      "timestamp\030\001 \002(\003\022\n\n\002id\030\002 \002(\005\"\035\n\017ResourceI" +
      "dProto\022\n\n\002id\030\001 \002(\005\"P\n\026AllocatedResourceP" +
      "roto\022\020\n\010hostname\030\001 \002(\t\022$\n\nresourceId\030\002 \002" +
      "(\0132\020.ResourceIdProto\"\235\001\n\033MpiJobAllocatio" +
      "nUpdateProto\022\022\n\nmpiJobName\030\001 \002(\t\0228\n\027newl" +
      "yAllocatedResources\030\002 \003(\0132\027.AllocatedRes" +
      "ourceProto\0220\n\020allocationStatus\030\003 \002(\0162\026.A" +
      "llocationStatusProto\"2\n\024StringStringMapP" +
      "roto\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t\"Y\n\022Loca",
      "lResourceProto\022\013\n\003url\030\001 \002(\t\022\025\n\rlocalized" +
      "Name\030\002 \002(\t\022\014\n\004size\030\003 \002(\003\022\021\n\ttimestamp\030\004 " +
      "\002(\003\"\211\002\n\032ResourceLaunchContextProto\022$\n\nre" +
      "sourceId\030\001 \002(\0132\020.ResourceIdProto\022\023\n\013comm" +
      "andline\030\002 \001(\t\022#\n\004envs\030\003 \003(\0132\025.StringStri" +
      "ngMapProto\022+\n\016localResources\030\004 \003(\0132\023.Loc" +
      "alResourceProto\022\034\n\024mpiDaemonCommandline\030" +
      "\005 \001(\t\022,\n\rmpiDaemonEnvs\030\006 \003(\0132\025.StringStr" +
      "ingMapProto\022\022\n\nlocalPeers\030\007 \001(\005\"%\n\025Creat" +
      "eJobRequestProto\022\014\n\004user\030\001 \002(\t\"4\n\026Create",
      "JobResponseProto\022\032\n\005jobId\030\001 \002(\0132\013.JobIdP" +
      "roto\"3\n\025HeartbeatRequestProto\022\032\n\005jobId\030\001" +
      " \002(\0132\013.JobIdProto\"G\n\026HeartbeatResponsePr" +
      "oto\022-\n\007updates\030\001 \003(\0132\034.MpiJobAllocationU" +
      "pdateProto\"|\n\025FinishJobRequestProto\022\032\n\005j" +
      "obId\030\001 \002(\0132\013.JobIdProto\022,\n\016finalJobStatu" +
      "s\030\002 \002(\0162\024.FinalJobStatusProto\022\031\n\021diagnos" +
      "ticMessage\030\003 \001(\t\"\030\n\026FinishJobResponsePro" +
      "to\"\312\001\n\024AllocateRequestProto\022\022\n\nmpiJobNam" +
      "e\030\001 \002(\t\022\022\n\nmemoryInMB\030\002 \001(\005\022\016\n\006vCores\030\003 ",
      "\001(\005\022\016\n\006policy\030\004 \001(\t\022\n\n\002np\030\005 \002(\005\022\025\n\rmaxPr" +
      "oPerNode\030\006 \001(\005\022\026\n\016minProcPerNode\030\007 \001(\005\022\023" +
      "\n\013timeoutInMs\030\010 \001(\003\022\032\n\005jobId\030\t \002(\0132\013.Job" +
      "IdProto\"\027\n\025AllocateResponseProto\"\201\001\n\022Lau" +
      "nchRequestProto\022\032\n\005jobId\030\001 \002(\0132\013.JobIdPr" +
      "oto\022\022\n\nmpiJobName\030\002 \002(\t\022;\n\026resourceLaunc" +
      "hContexts\030\003 \003(\0132\033.ResourceLaunchContextP" +
      "roto\"\025\n\023LaunchResponseProto\"G\n\025Terminate" +
      "RequestProto\022\032\n\005jobId\030\001 \002(\0132\013.JobIdProto" +
      "\022\022\n\nmpiJobName\030\002 \002(\t\"\030\n\026TerminateRespons",
      "eProto*^\n\025AllocationStatusProto\022\020\n\014A_ALL" +
      "OCATING\020\001\022\032\n\026A_ALLOCATION_SUCCEEDED\020\002\022\027\n" +
      "\023A_ALLOCATION_FAILED\020\003*H\n\023FinalJobStatus" +
      "Proto\022\021\n\rJOB_SUCCEEDED\020\001\022\016\n\nJOB_FAILED\020\002" +
      "\022\016\n\nJOB_UNKOWN\020\0032\202\003\n\030JobMasterProtocolSe" +
      "rvice\022<\n\tcreateJob\022\026.CreateJobRequestPro" +
      "to\032\027.CreateJobResponseProto\022<\n\theartbeat" +
      "\022\026.HeartbeatRequestProto\032\027.HeartbeatResp" +
      "onseProto\022<\n\tfinishJob\022\026.FinishJobReques" +
      "tProto\032\027.FinishJobResponseProto\0229\n\010alloc",
      "ate\022\025.AllocateRequestProto\032\026.AllocateRes" +
      "ponseProto\0223\n\006launch\022\023.LaunchRequestProt" +
      "o\032\024.LaunchResponseProto\022<\n\tterminate\022\026.T" +
      "erminateRequestProto\032\027.TerminateResponse" +
      "ProtoB7\n\034org.apache.hadoop.yarn.protoB\021J" +
      "obMasterProtocol\210\001\001\240\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_JobIdProto_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_JobIdProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_JobIdProto_descriptor,
              new java.lang.String[] { "Timestamp", "Id", });
          internal_static_ResourceIdProto_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_ResourceIdProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_ResourceIdProto_descriptor,
              new java.lang.String[] { "Id", });
          internal_static_AllocatedResourceProto_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_AllocatedResourceProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_AllocatedResourceProto_descriptor,
              new java.lang.String[] { "Hostname", "ResourceId", });
          internal_static_MpiJobAllocationUpdateProto_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_MpiJobAllocationUpdateProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_MpiJobAllocationUpdateProto_descriptor,
              new java.lang.String[] { "MpiJobName", "NewlyAllocatedResources", "AllocationStatus", });
          internal_static_StringStringMapProto_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_StringStringMapProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_StringStringMapProto_descriptor,
              new java.lang.String[] { "Key", "Value", });
          internal_static_LocalResourceProto_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_LocalResourceProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LocalResourceProto_descriptor,
              new java.lang.String[] { "Url", "LocalizedName", "Size", "Timestamp", });
          internal_static_ResourceLaunchContextProto_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_ResourceLaunchContextProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_ResourceLaunchContextProto_descriptor,
              new java.lang.String[] { "ResourceId", "Commandline", "Envs", "LocalResources", "MpiDaemonCommandline", "MpiDaemonEnvs", "LocalPeers", });
          internal_static_CreateJobRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_CreateJobRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_CreateJobRequestProto_descriptor,
              new java.lang.String[] { "User", });
          internal_static_CreateJobResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_CreateJobResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_CreateJobResponseProto_descriptor,
              new java.lang.String[] { "JobId", });
          internal_static_HeartbeatRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_HeartbeatRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_HeartbeatRequestProto_descriptor,
              new java.lang.String[] { "JobId", });
          internal_static_HeartbeatResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(10);
          internal_static_HeartbeatResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_HeartbeatResponseProto_descriptor,
              new java.lang.String[] { "Updates", });
          internal_static_FinishJobRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(11);
          internal_static_FinishJobRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_FinishJobRequestProto_descriptor,
              new java.lang.String[] { "JobId", "FinalJobStatus", "DiagnosticMessage", });
          internal_static_FinishJobResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(12);
          internal_static_FinishJobResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_FinishJobResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_AllocateRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(13);
          internal_static_AllocateRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_AllocateRequestProto_descriptor,
              new java.lang.String[] { "MpiJobName", "MemoryInMB", "VCores", "Policy", "Np", "MaxProPerNode", "MinProcPerNode", "TimeoutInMs", "JobId", });
          internal_static_AllocateResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(14);
          internal_static_AllocateResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_AllocateResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_LaunchRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(15);
          internal_static_LaunchRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LaunchRequestProto_descriptor,
              new java.lang.String[] { "JobId", "MpiJobName", "ResourceLaunchContexts", });
          internal_static_LaunchResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(16);
          internal_static_LaunchResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LaunchResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_TerminateRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(17);
          internal_static_TerminateRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_TerminateRequestProto_descriptor,
              new java.lang.String[] { "JobId", "MpiJobName", });
          internal_static_TerminateResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(18);
          internal_static_TerminateResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_TerminateResponseProto_descriptor,
              new java.lang.String[] { });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
